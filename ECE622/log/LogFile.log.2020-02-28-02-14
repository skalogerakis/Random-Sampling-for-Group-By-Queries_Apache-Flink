2020-02-28 02:14:05,122 [main] WARN  (FlinkKafkaProducer.java:667) - Property [transaction.timeout.ms] not specified. Setting it to 3600000 ms
2020-02-28 02:14:05,128 [main] WARN  (FlinkKafkaProducer.java:667) - Property [transaction.timeout.ms] not specified. Setting it to 3600000 ms
2020-02-28 02:14:05,330 [main] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion does not contain a setter for field one
2020-02-28 02:14:05,331 [main] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:05,333 [main] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion does not contain a setter for field one
2020-02-28 02:14:05,333 [main] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:05,745 [main] INFO  (LocalStreamEnvironment.java:108) - Running job on local embedded Flink mini cluster
2020-02-28 02:14:06,203 [main] INFO  (MiniCluster.java:253) - Starting Flink Mini Cluster
2020-02-28 02:14:06,206 [main] INFO  (MiniCluster.java:262) - Starting Metrics Registry
2020-02-28 02:14:06,276 [main] INFO  (MetricRegistryImpl.java:114) - No metrics reporter configured, no metrics will be exposed/reported.
2020-02-28 02:14:06,276 [main] INFO  (MiniCluster.java:266) - Starting RPC Service(s)
2020-02-28 02:14:07,144 [flink-akka.actor.default-dispatcher-3] INFO  (Slf4jLogger.scala:92) - Slf4jLogger started
2020-02-28 02:14:07,397 [main] INFO  (BootstrapTools.java:244) - Trying to start actor system at :0
2020-02-28 02:14:07,492 [flink-metrics-2] INFO  (Slf4jLogger.scala:92) - Slf4jLogger started
2020-02-28 02:14:07,517 [flink-metrics-2] INFO  (MarkerIgnoringBase.java:107) - Starting remoting
2020-02-28 02:14:07,656 [flink-metrics-2] INFO  (MarkerIgnoringBase.java:107) - Remoting started; listening on addresses :[akka.tcp://flink-metrics@127.0.1.1:34795]
2020-02-28 02:14:07,698 [main] INFO  (BootstrapTools.java:256) - Actor system started at akka.tcp://flink-metrics@127.0.1.1:34795
2020-02-28 02:14:07,706 [main] INFO  (AkkaRpcService.java:223) - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/MetricQueryService .
2020-02-28 02:14:07,719 [main] INFO  (MiniCluster.java:397) - Starting high-availability services
2020-02-28 02:14:07,734 [main] INFO  (BlobServer.java:141) - Created BLOB server storage directory /tmp/blobStore-8c38e9b2-bbdd-407b-85b9-55eae968c07a
2020-02-28 02:14:07,738 [main] INFO  (BlobServer.java:203) - Started BLOB server at 0.0.0.0:42821 - max concurrent requests: 50 - max backlog: 1000
2020-02-28 02:14:07,741 [main] INFO  (AbstractBlobCache.java:107) - Created BLOB cache storage directory /tmp/blobStore-153557b7-a0e3-411f-aaa3-0678bd4e8df0
2020-02-28 02:14:07,744 [main] INFO  (AbstractBlobCache.java:107) - Created BLOB cache storage directory /tmp/blobStore-3566b1fd-ac35-4ccd-bf96-385d0c5afde7
2020-02-28 02:14:07,744 [main] INFO  (MiniCluster.java:479) - Starting 1 TaskManger(s)
2020-02-28 02:14:07,748 [main] INFO  (TaskManagerRunner.java:351) - Starting TaskManager with ResourceID: 7fe72d87-e392-4de8-8d0a-e507450c23e6
2020-02-28 02:14:07,879 [main] INFO  (TaskManagerServices.java:519) - Temporary file directory '/tmp': total 439 GB, usable 336 GB (76.54% usable)
2020-02-28 02:14:07,883 [main] INFO  (FileChannelManagerImpl.java:76) - FileChannelManager uses directory /tmp/flink-io-e304bda9-634d-40a4-beb1-f5b58b96e4b0 for spill files.
2020-02-28 02:14:07,890 [main] INFO  (FileChannelManagerImpl.java:76) - FileChannelManager uses directory /tmp/flink-netty-shuffle-09db75cd-8de1-47eb-bc01-10190dd272ce for spill files.
2020-02-28 02:14:07,992 [main] INFO  (NetworkBufferPool.java:140) - Allocated 191 MB for network buffer pool (number of memory segments: 6113, bytes per segment: 32768).
2020-02-28 02:14:08,001 [main] INFO  (NettyShuffleEnvironment.java:283) - Starting the network environment and its components.
2020-02-28 02:14:08,003 [main] INFO  (KvStateService.java:89) - Starting the kvState service and its components.
2020-02-28 02:14:08,003 [main] INFO  (TaskManagerServices.java:364) - Limiting managed memory to 0.7 of the currently free heap space (1197 MB), memory will be allocated lazily.
2020-02-28 02:14:08,012 [main] INFO  (TaskManagerConfiguration.java:197) - Messages have a max timeout of 10000 ms
2020-02-28 02:14:08,022 [main] INFO  (AkkaRpcService.java:223) - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/taskmanager_0 .
2020-02-28 02:14:08,037 [flink-akka.actor.default-dispatcher-3] INFO  (JobLeaderService.java:125) - Start job leader service.
2020-02-28 02:14:08,038 [flink-akka.actor.default-dispatcher-3] INFO  (FileCache.java:107) - User file cache uses directory /tmp/flink-dist-cache-f894bed0-6a3a-468b-af51-ed6741fdde38
2020-02-28 02:14:08,075 [main] INFO  (RestServerEndpoint.java:136) - Starting rest endpoint.
2020-02-28 02:14:08,144 [main] WARN  (WebMonitorUtils.java:87) - Log file environment variable 'log.file' is not set.
2020-02-28 02:14:08,145 [main] WARN  (WebMonitorUtils.java:93) - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'Key: 'web.log.path' , default: null (fallback keys: [{key=jobmanager.web.log.path, isDeprecated=true}])'.
2020-02-28 02:14:08,156 [main] INFO  (DispatcherRestEndpoint.java:113) - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
2020-02-28 02:14:08,323 [main] INFO  (RestServerEndpoint.java:233) - Rest endpoint listening at localhost:42335
2020-02-28 02:14:08,325 [main] INFO  (EmbeddedLeaderService.java:300) - Proposing leadership to contender org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint@4038cd3a @ http://localhost:42335
2020-02-28 02:14:08,327 [mini-cluster-io-thread-1] INFO  (WebMonitorEndpoint.java:712) - http://localhost:42335 was granted leadership with leaderSessionID=6186549b-1808-4484-b115-817ae58a6aa6
2020-02-28 02:14:08,330 [mini-cluster-io-thread-1] INFO  (EmbeddedLeaderService.java:250) - Received confirmation of leadership for leader http://localhost:42335 , session=6186549b-1808-4484-b115-817ae58a6aa6
2020-02-28 02:14:08,345 [main] INFO  (AkkaRpcService.java:223) - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/resourcemanager .
2020-02-28 02:14:08,361 [main] INFO  (AkkaRpcService.java:223) - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/dispatcher .
2020-02-28 02:14:08,373 [flink-akka.actor.default-dispatcher-5] INFO  (EmbeddedLeaderService.java:300) - Proposing leadership to contender org.apache.flink.runtime.dispatcher.StandaloneDispatcher@8805d1f @ akka://flink/user/dispatcher
2020-02-28 02:14:08,374 [flink-akka.actor.default-dispatcher-2] INFO  (EmbeddedLeaderService.java:300) - Proposing leadership to contender org.apache.flink.runtime.resourcemanager.StandaloneResourceManager@663939f9 @ akka://flink/user/resourcemanager
2020-02-28 02:14:08,383 [flink-akka.actor.default-dispatcher-2] INFO  (Dispatcher.java:884) - Dispatcher akka://flink/user/dispatcher was granted leadership with fencing token fbccc181-8b67-435a-a072-5eb19f9b0e03
2020-02-28 02:14:08,384 [flink-akka.actor.default-dispatcher-5] INFO  (ResourceManager.java:925) - ResourceManager akka://flink/user/resourcemanager was granted leadership with fencing token 9089f79261a6b37a1a5ab0374640465e
2020-02-28 02:14:08,387 [flink-akka.actor.default-dispatcher-3] INFO  (Dispatcher.java:716) - Recovering all persisted jobs.
2020-02-28 02:14:08,390 [flink-akka.actor.default-dispatcher-5] INFO  (SlotManagerImpl.java:219) - Starting the SlotManager.
2020-02-28 02:14:08,393 [main] INFO  (MiniCluster.java:362) - Flink Mini Cluster started successfully
2020-02-28 02:14:08,396 [flink-akka.actor.default-dispatcher-4] INFO  (EmbeddedLeaderService.java:250) - Received confirmation of leadership for leader akka://flink/user/resourcemanager , session=1a5ab037-4640-465e-9089-f79261a6b37a
2020-02-28 02:14:08,405 [flink-akka.actor.default-dispatcher-3] INFO  (EmbeddedLeaderService.java:250) - Received confirmation of leadership for leader akka://flink/user/dispatcher , session=fbccc181-8b67-435a-a072-5eb19f9b0e03
2020-02-28 02:14:08,405 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:991) - Connecting to ResourceManager akka://flink/user/resourcemanager(9089f79261a6b37a1a5ab0374640465e).
2020-02-28 02:14:08,418 [flink-akka.actor.default-dispatcher-4] INFO  (RetryingRegistration.java:155) - Resolved ResourceManager address, beginning registration
2020-02-28 02:14:08,418 [flink-akka.actor.default-dispatcher-4] INFO  (RetryingRegistration.java:204) - Registration at ResourceManager attempt 1 (timeout=100ms)
2020-02-28 02:14:08,431 [flink-akka.actor.default-dispatcher-2] INFO  (Dispatcher.java:264) - Received JobGraph submission 160666712a30014a6455c625e0b51618 (Streaming FirstAlgorithmPass).
2020-02-28 02:14:08,432 [flink-akka.actor.default-dispatcher-2] INFO  (Dispatcher.java:321) - Submitting job 160666712a30014a6455c625e0b51618 (Streaming FirstAlgorithmPass).
2020-02-28 02:14:08,436 [flink-akka.actor.default-dispatcher-5] INFO  (ResourceManager.java:717) - Registering TaskManager with ResourceID 7fe72d87-e392-4de8-8d0a-e507450c23e6 (akka://flink/user/taskmanager_0) at ResourceManager
2020-02-28 02:14:08,439 [flink-akka.actor.default-dispatcher-2] INFO  (TaskExecutorToResourceManagerConnection.java:100) - Successful registration at resource manager akka://flink/user/resourcemanager under registration id 0a2b90c3f0d50b5807d50cf7cac0fcb9.
2020-02-28 02:14:08,456 [flink-akka.actor.default-dispatcher-3] INFO  (AkkaRpcService.java:223) - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/jobmanager_1 .
2020-02-28 02:14:08,466 [flink-akka.actor.default-dispatcher-3] INFO  (JobMaster.java:242) - Initializing job Streaming FirstAlgorithmPass (160666712a30014a6455c625e0b51618).
2020-02-28 02:14:08,487 [flink-akka.actor.default-dispatcher-3] INFO  (LegacyScheduler.java:171) - Using restart strategy NoRestartStrategy for Streaming FirstAlgorithmPass (160666712a30014a6455c625e0b51618).
2020-02-28 02:14:08,506 [flink-akka.actor.default-dispatcher-3] INFO  (ExecutionGraph.java:519) - Job recovers via failover strategy: full graph restart
2020-02-28 02:14:08,524 [flink-akka.actor.default-dispatcher-3] INFO  (ExecutionGraphBuilder.java:204) - Running initialization on master for job Streaming FirstAlgorithmPass (160666712a30014a6455c625e0b51618).
2020-02-28 02:14:08,524 [flink-akka.actor.default-dispatcher-3] INFO  (ExecutionGraphBuilder.java:222) - Successfully ran initialization on master in 0 ms.
2020-02-28 02:14:08,573 [flink-akka.actor.default-dispatcher-3] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,586 [flink-akka.actor.default-dispatcher-3] INFO  (EmbeddedLeaderService.java:300) - Proposing leadership to contender org.apache.flink.runtime.jobmaster.JobManagerRunner@37fe1ad9 @ akka://flink/user/jobmanager_1
2020-02-28 02:14:08,587 [mini-cluster-io-thread-1] INFO  (JobManagerRunner.java:313) - JobManager runner for job Streaming FirstAlgorithmPass (160666712a30014a6455c625e0b51618) was granted leadership with session id cd1c687d-d6b5-48a2-9752-f16a62baaa46 at akka://flink/user/jobmanager_1.
2020-02-28 02:14:08,591 [flink-akka.actor.default-dispatcher-5] INFO  (JobMaster.java:712) - Starting execution of job Streaming FirstAlgorithmPass (160666712a30014a6455c625e0b51618) under job master id 9752f16a62baaa46cd1c687dd6b548a2.
2020-02-28 02:14:08,593 [flink-akka.actor.default-dispatcher-5] INFO  (ExecutionGraph.java:1325) - Job Streaming FirstAlgorithmPass (160666712a30014a6455c625e0b51618) switched from state CREATED to RUNNING.
2020-02-28 02:14:08,596 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (9661654faedb77df167a0a6f0a108ca5) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,612 [flink-akka.actor.default-dispatcher-5] INFO  (SlotPoolImpl.java:369) - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{36e8a5f9be4f00fc031a1b7d6447c741}]
2020-02-28 02:14:08,619 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (46aeb721a4b20f67c76ce4309f026f1b) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,620 [flink-akka.actor.default-dispatcher-5] INFO  (SlotPoolImpl.java:369) - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{24338b111ff36bbc2b761c48501a9355}]
2020-02-28 02:14:08,620 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (0b9bc7902644eef62a7695460201e28a) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,621 [flink-akka.actor.default-dispatcher-5] INFO  (SlotPoolImpl.java:369) - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{1f41f049597ad529f6e1cdadacec9569}]
2020-02-28 02:14:08,621 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (a035c28b34c878fa471712e4eee709c5) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,621 [flink-akka.actor.default-dispatcher-5] INFO  (SlotPoolImpl.java:369) - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{18d27f2adb460098ef78920d80edb7fd}]
2020-02-28 02:14:08,621 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (e579e856236e92665a5b504bc7dc1e51) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,622 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (f99909add851acedad329181c6a09b53) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,622 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (c81757be7642c3872e2cc54ae1b51231) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,622 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (e9efd038b9167d5e1710e48b1a55bda8) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,622 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (71d790c54955937179734ec0aa3542f3) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,624 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (e1959e2a8d0e2b8a1b654524bc2ae2a0) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,624 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (0fbdb384c44b6550822d7ef6e5e3095c) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,624 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (c69c842b7ee393a67ba305b89f62a914) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,624 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (3a5f0f17d68bf02393b68d59f4ce6588) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,625 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (a1664bb75eab8cb2a7e6452ceb3308f5) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,625 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (ba79fa6280bc7bbc630fb6a2995621b2) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,625 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (d2295e000d1df9da237e1b51a121956c) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,625 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (dd59c63e0f1fe7d46d7ba3d3e39716b3) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,625 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (3edf8871d274005b5a3abd74f18fb7a1) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,626 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (e70ae8dc7ea98cbb4dfb7e0d1ed91f20) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,626 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (8984b7212976a7c132d8e86a2893b026) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,626 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (1/4) (20dabc062cd11225e32b99bbb92e354d) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,626 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (2/4) (c13ebf7a855b5a44219f6932da3831a2) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,627 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (3/4) (5f216adc18504b11c510dde5ffc7eabc) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,627 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (4/4) (f9228b723441d64d10319ef5af6e8010) switched from CREATED to SCHEDULED.
2020-02-28 02:14:08,629 [jobmanager-future-thread-1] INFO  (EmbeddedLeaderService.java:250) - Received confirmation of leadership for leader akka://flink/user/jobmanager_1 , session=cd1c687d-d6b5-48a2-9752-f16a62baaa46
2020-02-28 02:14:08,633 [flink-akka.actor.default-dispatcher-5] INFO  (JobMaster.java:936) - Connecting to ResourceManager akka://flink/user/resourcemanager(9089f79261a6b37a1a5ab0374640465e)
2020-02-28 02:14:08,635 [flink-akka.actor.default-dispatcher-5] INFO  (RetryingRegistration.java:155) - Resolved ResourceManager address, beginning registration
2020-02-28 02:14:08,635 [flink-akka.actor.default-dispatcher-5] INFO  (RetryingRegistration.java:204) - Registration at ResourceManager attempt 1 (timeout=100ms)
2020-02-28 02:14:08,637 [flink-akka.actor.default-dispatcher-2] INFO  (ResourceManager.java:306) - Registering job manager 9752f16a62baaa46cd1c687dd6b548a2@akka://flink/user/jobmanager_1 for job 160666712a30014a6455c625e0b51618.
2020-02-28 02:14:08,643 [flink-akka.actor.default-dispatcher-4] INFO  (ResourceManager.java:661) - Registered job manager 9752f16a62baaa46cd1c687dd6b548a2@akka://flink/user/jobmanager_1 for job 160666712a30014a6455c625e0b51618.
2020-02-28 02:14:08,645 [flink-akka.actor.default-dispatcher-5] INFO  (JobMaster.java:958) - JobManager successfully registered at ResourceManager, leader id: 9089f79261a6b37a1a5ab0374640465e.
2020-02-28 02:14:08,646 [flink-akka.actor.default-dispatcher-5] INFO  (SlotPoolImpl.java:319) - Requesting new slot [SlotRequestId{36e8a5f9be4f00fc031a1b7d6447c741}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
2020-02-28 02:14:08,647 [flink-akka.actor.default-dispatcher-2] INFO  (ResourceManager.java:441) - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job 160666712a30014a6455c625e0b51618 with allocation id 7d14a59c8310cff46d0e003a2c61f4a1.
2020-02-28 02:14:08,647 [flink-akka.actor.default-dispatcher-5] INFO  (SlotPoolImpl.java:319) - Requesting new slot [SlotRequestId{24338b111ff36bbc2b761c48501a9355}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
2020-02-28 02:14:08,648 [flink-akka.actor.default-dispatcher-5] INFO  (SlotPoolImpl.java:319) - Requesting new slot [SlotRequestId{1f41f049597ad529f6e1cdadacec9569}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
2020-02-28 02:14:08,648 [flink-akka.actor.default-dispatcher-5] INFO  (SlotPoolImpl.java:319) - Requesting new slot [SlotRequestId{18d27f2adb460098ef78920d80edb7fd}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
2020-02-28 02:14:08,653 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:822) - Receive slot request 7d14a59c8310cff46d0e003a2c61f4a1 for job 160666712a30014a6455c625e0b51618 from resource manager with leader id 9089f79261a6b37a1a5ab0374640465e.
2020-02-28 02:14:08,654 [flink-akka.actor.default-dispatcher-2] INFO  (ResourceManager.java:441) - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job 160666712a30014a6455c625e0b51618 with allocation id 23c1bbbf5b88f6d0835e51aa49f24d77.
2020-02-28 02:14:08,655 [flink-akka.actor.default-dispatcher-2] INFO  (ResourceManager.java:441) - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job 160666712a30014a6455c625e0b51618 with allocation id a5478989d7ca7fe2a2ac35a024abc5ff.
2020-02-28 02:14:08,656 [flink-akka.actor.default-dispatcher-2] INFO  (ResourceManager.java:441) - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job 160666712a30014a6455c625e0b51618 with allocation id e5eb0b0dab8377f1575d1f38e254d98c.
2020-02-28 02:14:08,658 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:834) - Allocated slot for 7d14a59c8310cff46d0e003a2c61f4a1.
2020-02-28 02:14:08,658 [flink-akka.actor.default-dispatcher-5] INFO  (JobLeaderService.java:193) - Add job 160666712a30014a6455c625e0b51618 for job leader monitoring.
2020-02-28 02:14:08,660 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:822) - Receive slot request 23c1bbbf5b88f6d0835e51aa49f24d77 for job 160666712a30014a6455c625e0b51618 from resource manager with leader id 9089f79261a6b37a1a5ab0374640465e.
2020-02-28 02:14:08,660 [mini-cluster-io-thread-4] INFO  (JobLeaderService.java:333) - Try to register at job manager akka://flink/user/jobmanager_1 with leader id cd1c687d-d6b5-48a2-9752-f16a62baaa46.
2020-02-28 02:14:08,660 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:834) - Allocated slot for 23c1bbbf5b88f6d0835e51aa49f24d77.
2020-02-28 02:14:08,661 [flink-akka.actor.default-dispatcher-5] INFO  (JobLeaderService.java:193) - Add job 160666712a30014a6455c625e0b51618 for job leader monitoring.
2020-02-28 02:14:08,662 [flink-akka.actor.default-dispatcher-4] INFO  (RetryingRegistration.java:155) - Resolved JobManager address, beginning registration
2020-02-28 02:14:08,662 [mini-cluster-io-thread-1] INFO  (JobLeaderService.java:333) - Try to register at job manager akka://flink/user/jobmanager_1 with leader id cd1c687d-d6b5-48a2-9752-f16a62baaa46.
2020-02-28 02:14:08,662 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:822) - Receive slot request a5478989d7ca7fe2a2ac35a024abc5ff for job 160666712a30014a6455c625e0b51618 from resource manager with leader id 9089f79261a6b37a1a5ab0374640465e.
2020-02-28 02:14:08,663 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:834) - Allocated slot for a5478989d7ca7fe2a2ac35a024abc5ff.
2020-02-28 02:14:08,663 [flink-akka.actor.default-dispatcher-5] INFO  (JobLeaderService.java:193) - Add job 160666712a30014a6455c625e0b51618 for job leader monitoring.
2020-02-28 02:14:08,663 [mini-cluster-io-thread-3] INFO  (JobLeaderService.java:333) - Try to register at job manager akka://flink/user/jobmanager_1 with leader id cd1c687d-d6b5-48a2-9752-f16a62baaa46.
2020-02-28 02:14:08,664 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:822) - Receive slot request e5eb0b0dab8377f1575d1f38e254d98c for job 160666712a30014a6455c625e0b51618 from resource manager with leader id 9089f79261a6b37a1a5ab0374640465e.
2020-02-28 02:14:08,663 [flink-akka.actor.default-dispatcher-3] INFO  (RetryingRegistration.java:155) - Resolved JobManager address, beginning registration
2020-02-28 02:14:08,664 [flink-akka.actor.default-dispatcher-2] INFO  (RetryingRegistration.java:155) - Resolved JobManager address, beginning registration
2020-02-28 02:14:08,665 [flink-akka.actor.default-dispatcher-2] INFO  (RetryingRegistration.java:204) - Registration at JobManager attempt 1 (timeout=100ms)
2020-02-28 02:14:08,665 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:834) - Allocated slot for e5eb0b0dab8377f1575d1f38e254d98c.
2020-02-28 02:14:08,665 [flink-akka.actor.default-dispatcher-5] INFO  (JobLeaderService.java:193) - Add job 160666712a30014a6455c625e0b51618 for job leader monitoring.
2020-02-28 02:14:08,666 [mini-cluster-io-thread-2] INFO  (JobLeaderService.java:333) - Try to register at job manager akka://flink/user/jobmanager_1 with leader id cd1c687d-d6b5-48a2-9752-f16a62baaa46.
2020-02-28 02:14:08,667 [flink-akka.actor.default-dispatcher-2] INFO  (RetryingRegistration.java:155) - Resolved JobManager address, beginning registration
2020-02-28 02:14:08,668 [flink-akka.actor.default-dispatcher-2] INFO  (RetryingRegistration.java:204) - Registration at JobManager attempt 1 (timeout=100ms)
2020-02-28 02:14:08,670 [flink-akka.actor.default-dispatcher-4] INFO  (JobLeaderService.java:382) - Successful registration at job manager akka://flink/user/jobmanager_1 for job 160666712a30014a6455c625e0b51618.
2020-02-28 02:14:08,671 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:1227) - Establish JobManager connection for job 160666712a30014a6455c625e0b51618.
2020-02-28 02:14:08,674 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:1128) - Offer reserved slots to the leader of job 160666712a30014a6455c625e0b51618.
2020-02-28 02:14:08,690 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (9661654faedb77df167a0a6f0a108ca5) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,690 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (1/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,701 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (46aeb721a4b20f67c76ce4309f026f1b) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,701 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (2/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,702 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (0b9bc7902644eef62a7695460201e28a) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,703 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (3/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,703 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (a035c28b34c878fa471712e4eee709c5) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,704 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (4/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,704 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (e579e856236e92665a5b504bc7dc1e51) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,705 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,706 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (f99909add851acedad329181c6a09b53) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,706 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,706 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (c81757be7642c3872e2cc54ae1b51231) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,707 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,708 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (e9efd038b9167d5e1710e48b1a55bda8) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,708 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,708 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (71d790c54955937179734ec0aa3542f3) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,709 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,710 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (e1959e2a8d0e2b8a1b654524bc2ae2a0) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,711 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,711 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (0fbdb384c44b6550822d7ef6e5e3095c) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,712 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,712 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (c69c842b7ee393a67ba305b89f62a914) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,713 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,716 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (3a5f0f17d68bf02393b68d59f4ce6588) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,717 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,718 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (a1664bb75eab8cb2a7e6452ceb3308f5) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,719 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,719 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (ba79fa6280bc7bbc630fb6a2995621b2) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,720 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,721 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (d2295e000d1df9da237e1b51a121956c) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,721 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,722 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (dd59c63e0f1fe7d46d7ba3d3e39716b3) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,722 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,724 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (3edf8871d274005b5a3abd74f18fb7a1) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,724 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,725 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (e70ae8dc7ea98cbb4dfb7e0d1ed91f20) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,727 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,728 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (8984b7212976a7c132d8e86a2893b026) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,729 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,729 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Flat Map (1/4) (20dabc062cd11225e32b99bbb92e354d) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,729 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Flat Map (1/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,730 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Flat Map (2/4) (c13ebf7a855b5a44219f6932da3831a2) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,730 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Flat Map (2/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,731 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Flat Map (3/4) (5f216adc18504b11c510dde5ffc7eabc) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,731 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Flat Map (3/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,731 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Flat Map (4/4) (f9228b723441d64d10319ef5af6e8010) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:08,732 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:713) - Deploying Flat Map (4/4) (attempt #0) to 7fe72d87-e392-4de8-8d0a-e507450c23e6 @ localhost (dataPort=-1)
2020-02-28 02:14:08,745 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (1/4).
2020-02-28 02:14:08,746 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (9661654faedb77df167a0a6f0a108ca5) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,746 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (1/4) (9661654faedb77df167a0a6f0a108ca5) [DEPLOYING]
2020-02-28 02:14:08,754 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (2/4).
2020-02-28 02:14:08,768 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (3/4).
2020-02-28 02:14:08,769 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (1/4) (9661654faedb77df167a0a6f0a108ca5) [DEPLOYING].
2020-02-28 02:14:08,771 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (1/4) (9661654faedb77df167a0a6f0a108ca5) [DEPLOYING].
2020-02-28 02:14:08,774 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (0b9bc7902644eef62a7695460201e28a) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,775 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (3/4) (0b9bc7902644eef62a7695460201e28a) [DEPLOYING]
2020-02-28 02:14:08,768 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (46aeb721a4b20f67c76ce4309f026f1b) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,776 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (2/4) (46aeb721a4b20f67c76ce4309f026f1b) [DEPLOYING]
2020-02-28 02:14:08,776 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (2/4) (46aeb721a4b20f67c76ce4309f026f1b) [DEPLOYING].
2020-02-28 02:14:08,778 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (2/4) (46aeb721a4b20f67c76ce4309f026f1b) [DEPLOYING].
2020-02-28 02:14:08,779 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (4/4).
2020-02-28 02:14:08,789 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (9661654faedb77df167a0a6f0a108ca5) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,790 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (9661654faedb77df167a0a6f0a108ca5) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,790 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (3/4) (0b9bc7902644eef62a7695460201e28a) [DEPLOYING].
2020-02-28 02:14:08,792 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (3/4) (0b9bc7902644eef62a7695460201e28a) [DEPLOYING].
2020-02-28 02:14:08,793 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (0b9bc7902644eef62a7695460201e28a) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,792 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (46aeb721a4b20f67c76ce4309f026f1b) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,795 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,794 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (a035c28b34c878fa471712e4eee709c5) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,800 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (4/4) (a035c28b34c878fa471712e4eee709c5) [DEPLOYING]
2020-02-28 02:14:08,800 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (4/4) (a035c28b34c878fa471712e4eee709c5) [DEPLOYING].
2020-02-28 02:14:08,794 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,794 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,796 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (0b9bc7902644eef62a7695460201e28a) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,803 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (46aeb721a4b20f67c76ce4309f026f1b) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,806 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (4/4) (a035c28b34c878fa471712e4eee709c5) [DEPLOYING].
2020-02-28 02:14:08,808 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (a035c28b34c878fa471712e4eee709c5) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,808 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,808 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (a035c28b34c878fa471712e4eee709c5) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,817 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4).
2020-02-28 02:14:08,821 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (e579e856236e92665a5b504bc7dc1e51) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,822 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (e579e856236e92665a5b504bc7dc1e51) [DEPLOYING]
2020-02-28 02:14:08,822 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (e579e856236e92665a5b504bc7dc1e51) [DEPLOYING].
2020-02-28 02:14:08,830 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4).
2020-02-28 02:14:08,831 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (f99909add851acedad329181c6a09b53) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,831 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (f99909add851acedad329181c6a09b53) [DEPLOYING]
2020-02-28 02:14:08,832 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (f99909add851acedad329181c6a09b53) [DEPLOYING].
2020-02-28 02:14:08,832 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (e579e856236e92665a5b504bc7dc1e51) [DEPLOYING].
2020-02-28 02:14:08,838 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (f99909add851acedad329181c6a09b53) [DEPLOYING].
2020-02-28 02:14:08,839 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4).
2020-02-28 02:14:08,842 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4).
2020-02-28 02:14:08,843 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (c81757be7642c3872e2cc54ae1b51231) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,843 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (c81757be7642c3872e2cc54ae1b51231) [DEPLOYING]
2020-02-28 02:14:08,843 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (c81757be7642c3872e2cc54ae1b51231) [DEPLOYING].
2020-02-28 02:14:08,843 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (e9efd038b9167d5e1710e48b1a55bda8) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,844 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (e9efd038b9167d5e1710e48b1a55bda8) [DEPLOYING]
2020-02-28 02:14:08,844 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (e9efd038b9167d5e1710e48b1a55bda8) [DEPLOYING].
2020-02-28 02:14:08,849 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (e9efd038b9167d5e1710e48b1a55bda8) [DEPLOYING].
2020-02-28 02:14:08,849 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (c81757be7642c3872e2cc54ae1b51231) [DEPLOYING].
2020-02-28 02:14:08,860 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (f99909add851acedad329181c6a09b53) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,868 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,870 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4).
2020-02-28 02:14:08,871 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (f99909add851acedad329181c6a09b53) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,865 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (e579e856236e92665a5b504bc7dc1e51) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,872 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (e579e856236e92665a5b504bc7dc1e51) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,873 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,883 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:08,883 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:08,891 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:08,891 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:08,891 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:08,891 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:08,888 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:08,891 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (71d790c54955937179734ec0aa3542f3) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,892 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:08,892 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (71d790c54955937179734ec0aa3542f3) [DEPLOYING]
2020-02-28 02:14:08,892 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (71d790c54955937179734ec0aa3542f3) [DEPLOYING].
2020-02-28 02:14:08,896 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:08,896 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:08,897 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 0/4 - no state to restore
2020-02-28 02:14:08,897 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 1/4 - no state to restore
2020-02-28 02:14:08,899 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (e9efd038b9167d5e1710e48b1a55bda8) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,900 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (e9efd038b9167d5e1710e48b1a55bda8) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,900 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,905 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (71d790c54955937179734ec0aa3542f3) [DEPLOYING].
2020-02-28 02:14:08,906 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 2 has no restore state.
2020-02-28 02:14:08,910 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4).
2020-02-28 02:14:08,910 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (c81757be7642c3872e2cc54ae1b51231) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,910 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,910 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (c81757be7642c3872e2cc54ae1b51231) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,922 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 3 has no restore state.
2020-02-28 02:14:08,922 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 1 has no restore state.
2020-02-28 02:14:08,922 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 0 has no restore state.
2020-02-28 02:14:08,922 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (e1959e2a8d0e2b8a1b654524bc2ae2a0) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,923 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (e1959e2a8d0e2b8a1b654524bc2ae2a0) [DEPLOYING]
2020-02-28 02:14:08,924 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4).
2020-02-28 02:14:08,924 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (e1959e2a8d0e2b8a1b654524bc2ae2a0) [DEPLOYING].
2020-02-28 02:14:08,929 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (e1959e2a8d0e2b8a1b654524bc2ae2a0) [DEPLOYING].
2020-02-28 02:14:08,930 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (0fbdb384c44b6550822d7ef6e5e3095c) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,930 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (0fbdb384c44b6550822d7ef6e5e3095c) [DEPLOYING]
2020-02-28 02:14:08,930 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (0fbdb384c44b6550822d7ef6e5e3095c) [DEPLOYING].
2020-02-28 02:14:08,931 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:08,931 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 3/4 - no state to restore
2020-02-28 02:14:08,933 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4).
2020-02-28 02:14:08,933 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (0fbdb384c44b6550822d7ef6e5e3095c) [DEPLOYING].
2020-02-28 02:14:08,933 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:08,933 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 2/4 - no state to restore
2020-02-28 02:14:08,945 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (71d790c54955937179734ec0aa3542f3) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,945 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (3a5f0f17d68bf02393b68d59f4ce6588) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,945 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (3a5f0f17d68bf02393b68d59f4ce6588) [DEPLOYING]
2020-02-28 02:14:08,945 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (71d790c54955937179734ec0aa3542f3) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,960 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (3a5f0f17d68bf02393b68d59f4ce6588) [DEPLOYING].
2020-02-28 02:14:08,961 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (3a5f0f17d68bf02393b68d59f4ce6588) [DEPLOYING].
2020-02-28 02:14:08,960 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,960 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4).
2020-02-28 02:14:08,965 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4).
2020-02-28 02:14:08,966 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (a1664bb75eab8cb2a7e6452ceb3308f5) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,966 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (a1664bb75eab8cb2a7e6452ceb3308f5) [DEPLOYING]
2020-02-28 02:14:08,966 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (a1664bb75eab8cb2a7e6452ceb3308f5) [DEPLOYING].
2020-02-28 02:14:08,967 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (a1664bb75eab8cb2a7e6452ceb3308f5) [DEPLOYING].
2020-02-28 02:14:08,967 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (ba79fa6280bc7bbc630fb6a2995621b2) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,967 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (ba79fa6280bc7bbc630fb6a2995621b2) [DEPLOYING]
2020-02-28 02:14:08,967 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (ba79fa6280bc7bbc630fb6a2995621b2) [DEPLOYING].
2020-02-28 02:14:08,970 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (ba79fa6280bc7bbc630fb6a2995621b2) [DEPLOYING].
2020-02-28 02:14:08,972 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (3a5f0f17d68bf02393b68d59f4ce6588) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,972 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4).
2020-02-28 02:14:08,972 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,972 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (3a5f0f17d68bf02393b68d59f4ce6588) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,976 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4).
2020-02-28 02:14:08,977 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (c69c842b7ee393a67ba305b89f62a914) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,977 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (c69c842b7ee393a67ba305b89f62a914) [DEPLOYING]
2020-02-28 02:14:08,977 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (c69c842b7ee393a67ba305b89f62a914) [DEPLOYING].
2020-02-28 02:14:08,979 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (d2295e000d1df9da237e1b51a121956c) switched from CREATED to DEPLOYING.
2020-02-28 02:14:08,979 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (d2295e000d1df9da237e1b51a121956c) [DEPLOYING]
2020-02-28 02:14:08,979 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (d2295e000d1df9da237e1b51a121956c) [DEPLOYING].
2020-02-28 02:14:08,981 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (e1959e2a8d0e2b8a1b654524bc2ae2a0) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,981 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (c69c842b7ee393a67ba305b89f62a914) [DEPLOYING].
2020-02-28 02:14:08,982 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (e1959e2a8d0e2b8a1b654524bc2ae2a0) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:08,980 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] WARN  (TaskMetricGroup.java:143) - The operator name Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) exceeded the 80 characters length limit and was truncated.
2020-02-28 02:14:08,982 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:08,981 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (d2295e000d1df9da237e1b51a121956c) [DEPLOYING].
2020-02-28 02:14:08,986 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:08,988 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:08,989 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4).
2020-02-28 02:14:08,995 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:08,994 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:08,994 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:08,986 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,015 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (a1664bb75eab8cb2a7e6452ceb3308f5) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,016 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,017 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (a1664bb75eab8cb2a7e6452ceb3308f5) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,015 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (ba79fa6280bc7bbc630fb6a2995621b2) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,026 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,002 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4).
2020-02-28 02:14:09,000 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (dd59c63e0f1fe7d46d7ba3d3e39716b3) switched from CREATED to DEPLOYING.
2020-02-28 02:14:09,028 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (dd59c63e0f1fe7d46d7ba3d3e39716b3) [DEPLOYING]
2020-02-28 02:14:09,028 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (dd59c63e0f1fe7d46d7ba3d3e39716b3) [DEPLOYING].
2020-02-28 02:14:09,029 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (dd59c63e0f1fe7d46d7ba3d3e39716b3) [DEPLOYING].
2020-02-28 02:14:08,995 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:08,990 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:09,026 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (ba79fa6280bc7bbc630fb6a2995621b2) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,017 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (d2295e000d1df9da237e1b51a121956c) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,051 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,045 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] WARN  (TaskMetricGroup.java:143) - The operator name Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) exceeded the 80 characters length limit and was truncated.
2020-02-28 02:14:09,042 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] WARN  (TaskMetricGroup.java:143) - The operator name Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) exceeded the 80 characters length limit and was truncated.
2020-02-28 02:14:09,041 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4).
2020-02-28 02:14:09,041 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (3edf8871d274005b5a3abd74f18fb7a1) switched from CREATED to DEPLOYING.
2020-02-28 02:14:09,051 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (d2295e000d1df9da237e1b51a121956c) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,059 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (3edf8871d274005b5a3abd74f18fb7a1) [DEPLOYING]
2020-02-28 02:14:09,060 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (3edf8871d274005b5a3abd74f18fb7a1) [DEPLOYING].
2020-02-28 02:14:09,060 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (3edf8871d274005b5a3abd74f18fb7a1) [DEPLOYING].
2020-02-28 02:14:09,062 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4).
2020-02-28 02:14:09,065 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (e70ae8dc7ea98cbb4dfb7e0d1ed91f20) switched from CREATED to DEPLOYING.
2020-02-28 02:14:09,064 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] WARN  (TaskMetricGroup.java:143) - The operator name Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) exceeded the 80 characters length limit and was truncated.
2020-02-28 02:14:09,066 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (8984b7212976a7c132d8e86a2893b026) switched from CREATED to DEPLOYING.
2020-02-28 02:14:09,067 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (8984b7212976a7c132d8e86a2893b026) [DEPLOYING]
2020-02-28 02:14:09,067 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (8984b7212976a7c132d8e86a2893b026) [DEPLOYING].
2020-02-28 02:14:09,067 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (e70ae8dc7ea98cbb4dfb7e0d1ed91f20) [DEPLOYING]
2020-02-28 02:14:09,067 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (8984b7212976a7c132d8e86a2893b026) [DEPLOYING].
2020-02-28 02:14:09,067 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (e70ae8dc7ea98cbb4dfb7e0d1ed91f20) [DEPLOYING].
2020-02-28 02:14:09,068 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (e70ae8dc7ea98cbb4dfb7e0d1ed91f20) [DEPLOYING].
2020-02-28 02:14:09,072 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Flat Map (1/4).
2020-02-28 02:14:09,072 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (dd59c63e0f1fe7d46d7ba3d3e39716b3) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,073 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,074 [Flat Map (1/4)] INFO  (Task.java:958) - Flat Map (1/4) (20dabc062cd11225e32b99bbb92e354d) switched from CREATED to DEPLOYING.
2020-02-28 02:14:09,073 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (dd59c63e0f1fe7d46d7ba3d3e39716b3) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,074 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (3edf8871d274005b5a3abd74f18fb7a1) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,075 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,075 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (3edf8871d274005b5a3abd74f18fb7a1) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,074 [Flat Map (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (1/4) (20dabc062cd11225e32b99bbb92e354d) [DEPLOYING]
2020-02-28 02:14:09,074 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (8984b7212976a7c132d8e86a2893b026) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,077 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,077 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (0fbdb384c44b6550822d7ef6e5e3095c) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,077 [Flat Map (1/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (1/4) (20dabc062cd11225e32b99bbb92e354d) [DEPLOYING].
2020-02-28 02:14:09,080 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,076 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (c69c842b7ee393a67ba305b89f62a914) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,081 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,083 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,074 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (e70ae8dc7ea98cbb4dfb7e0d1ed91f20) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,080 [Flat Map (1/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (1/4) (20dabc062cd11225e32b99bbb92e354d) [DEPLOYING].
2020-02-28 02:14:09,077 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (8984b7212976a7c132d8e86a2893b026) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,085 [Flat Map (1/4)] INFO  (Task.java:958) - Flat Map (1/4) (20dabc062cd11225e32b99bbb92e354d) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,086 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (0fbdb384c44b6550822d7ef6e5e3095c) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,086 [Flat Map (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,086 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (c69c842b7ee393a67ba305b89f62a914) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,087 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Flat Map (1/4) (20dabc062cd11225e32b99bbb92e354d) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,088 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (e70ae8dc7ea98cbb4dfb7e0d1ed91f20) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,089 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,094 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Flat Map (2/4).
2020-02-28 02:14:09,095 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Flat Map (3/4).
2020-02-28 02:14:09,095 [flink-akka.actor.default-dispatcher-5] INFO  (TaskSlotTable.java:242) - Activate slot 7d14a59c8310cff46d0e003a2c61f4a1.
2020-02-28 02:14:09,096 [flink-akka.actor.default-dispatcher-5] INFO  (TaskSlotTable.java:242) - Activate slot e5eb0b0dab8377f1575d1f38e254d98c.
2020-02-28 02:14:09,097 [flink-akka.actor.default-dispatcher-5] INFO  (TaskSlotTable.java:242) - Activate slot a5478989d7ca7fe2a2ac35a024abc5ff.
2020-02-28 02:14:09,097 [flink-akka.actor.default-dispatcher-5] INFO  (TaskSlotTable.java:242) - Activate slot 23c1bbbf5b88f6d0835e51aa49f24d77.
2020-02-28 02:14:09,098 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:592) - Received task Flat Map (4/4).
2020-02-28 02:14:09,099 [Flat Map (3/4)] INFO  (Task.java:958) - Flat Map (3/4) (5f216adc18504b11c510dde5ffc7eabc) switched from CREATED to DEPLOYING.
2020-02-28 02:14:09,099 [Flat Map (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (3/4) (5f216adc18504b11c510dde5ffc7eabc) [DEPLOYING]
2020-02-28 02:14:09,083 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,101 [Flat Map (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,083 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,101 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,084 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,084 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,084 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,107 [Flat Map (2/4)] INFO  (Task.java:958) - Flat Map (2/4) (c13ebf7a855b5a44219f6932da3831a2) switched from CREATED to DEPLOYING.
2020-02-28 02:14:09,118 [Flat Map (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (2/4) (c13ebf7a855b5a44219f6932da3831a2) [DEPLOYING]
2020-02-28 02:14:09,118 [Flat Map (2/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (2/4) (c13ebf7a855b5a44219f6932da3831a2) [DEPLOYING].
2020-02-28 02:14:09,118 [Flat Map (2/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (2/4) (c13ebf7a855b5a44219f6932da3831a2) [DEPLOYING].
2020-02-28 02:14:09,119 [Flat Map (2/4)] INFO  (Task.java:958) - Flat Map (2/4) (c13ebf7a855b5a44219f6932da3831a2) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,119 [Flat Map (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,120 [Flat Map (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,107 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,105 [Flat Map (3/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (3/4) (5f216adc18504b11c510dde5ffc7eabc) [DEPLOYING].
2020-02-28 02:14:09,119 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (2/4) (c13ebf7a855b5a44219f6932da3831a2) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,116 [Flat Map (4/4)] INFO  (Task.java:958) - Flat Map (4/4) (f9228b723441d64d10319ef5af6e8010) switched from CREATED to DEPLOYING.
2020-02-28 02:14:09,121 [Flat Map (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (4/4) (f9228b723441d64d10319ef5af6e8010) [DEPLOYING]
2020-02-28 02:14:09,121 [Flat Map (4/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (4/4) (f9228b723441d64d10319ef5af6e8010) [DEPLOYING].
2020-02-28 02:14:09,121 [Flat Map (3/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (3/4) (5f216adc18504b11c510dde5ffc7eabc) [DEPLOYING].
2020-02-28 02:14:09,121 [Flat Map (3/4)] INFO  (Task.java:958) - Flat Map (3/4) (5f216adc18504b11c510dde5ffc7eabc) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,122 [Flat Map (4/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (4/4) (f9228b723441d64d10319ef5af6e8010) [DEPLOYING].
2020-02-28 02:14:09,122 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Flat Map (3/4) (5f216adc18504b11c510dde5ffc7eabc) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,122 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:09,122 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 1/4 - no state to restore
2020-02-28 02:14:09,122 [Flat Map (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,122 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:09,122 [Flat Map (4/4)] INFO  (Task.java:958) - Flat Map (4/4) (f9228b723441d64d10319ef5af6e8010) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,123 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 0/4 - no state to restore
2020-02-28 02:14:09,123 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (4/4) (f9228b723441d64d10319ef5af6e8010) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:09,123 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:09,124 [Flat Map (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:09,123 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:09,138 [Flat Map (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,144 [Flat Map (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,137 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:09,128 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:09,144 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 3/4 - no state to restore
2020-02-28 02:14:09,144 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 2/4 - no state to restore
2020-02-28 02:14:09,145 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:09,145 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:09,191 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,197 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,197 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849189
2020-02-28 02:14:09,206 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (2/4) to produce into default topic _input1
2020-02-28 02:14:09,206 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,219 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,219 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849198
2020-02-28 02:14:09,220 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:09,220 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:09,220 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 1 has no restore state.
2020-02-28 02:14:09,221 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,253 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (1/4) to produce into default topic output
2020-02-28 02:14:09,253 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,254 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,255 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,256 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849193
2020-02-28 02:14:09,264 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (2/4) to produce into default topic output
2020-02-28 02:14:09,264 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,266 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,266 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849219
2020-02-28 02:14:09,265 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,269 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (4/4) to produce into default topic output
2020-02-28 02:14:09,270 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,270 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,272 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,272 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849217
2020-02-28 02:14:09,273 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (4/4) to produce into default topic _input1
2020-02-28 02:14:09,274 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:09,276 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:09,277 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 3 has no restore state.
2020-02-28 02:14:09,278 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,273 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,278 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,278 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849217
2020-02-28 02:14:09,279 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (3/4) to produce into default topic output
2020-02-28 02:14:09,279 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:09,279 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,281 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,281 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849209
2020-02-28 02:14:09,281 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (1/4) to produce into default topic _input1
2020-02-28 02:14:09,282 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,282 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,282 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849209
2020-02-28 02:14:09,282 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (3/4) to produce into default topic _input1
2020-02-28 02:14:09,282 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:09,283 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:09,283 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 0 has no restore state.
2020-02-28 02:14:09,284 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,288 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:09,289 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:09,289 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 2 has no restore state.
2020-02-28 02:14:09,290 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,290 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,293 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,293 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849276
2020-02-28 02:14:09,293 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,297 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,297 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849271
2020-02-28 02:14:09,299 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,299 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,299 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849271
2020-02-28 02:14:09,301 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,301 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,302 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849266
2020-02-28 02:14:09,304 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,304 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,304 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849266
2020-02-28 02:14:09,305 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,305 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,305 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849303
2020-02-28 02:14:09,305 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,306 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,306 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849293
2020-02-28 02:14:09,313 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,313 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,313 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849313
2020-02-28 02:14:09,675 [kafka-producer-network-thread | producer-2] INFO  (Metadata.java:261) - [Producer clientId=producer-2] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,683 [kafka-producer-network-thread | producer-7] INFO  (Metadata.java:261) - [Producer clientId=producer-7] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,683 [kafka-producer-network-thread | producer-1] INFO  (Metadata.java:261) - [Producer clientId=producer-1] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,682 [kafka-producer-network-thread | producer-8] INFO  (Metadata.java:261) - [Producer clientId=producer-8] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,682 [kafka-producer-network-thread | producer-3] INFO  (Metadata.java:261) - [Producer clientId=producer-3] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,681 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-2, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,684 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-7, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,680 [kafka-producer-network-thread | producer-6] INFO  (Metadata.java:261) - [Producer clientId=producer-6] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,679 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-4, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,678 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-5, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,686 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-3, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,675 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-1, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,686 [kafka-producer-network-thread | producer-4] INFO  (Metadata.java:261) - [Producer clientId=producer-4] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,686 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-6, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,684 [kafka-producer-network-thread | producer-5] INFO  (Metadata.java:261) - [Producer clientId=producer-5] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,683 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-8, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,688 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 0 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='input1', partition=7}, KafkaTopicPartition{topic='input1', partition=3}]
2020-02-28 02:14:09,688 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 1 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='input1', partition=4}, KafkaTopicPartition{topic='input1', partition=0}]
2020-02-28 02:14:09,689 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 2 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='input1', partition=5}, KafkaTopicPartition{topic='input1', partition=1}]
2020-02-28 02:14:09,689 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 3 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='input1', partition=6}, KafkaTopicPartition{topic='input1', partition=2}]
2020-02-28 02:14:09,688 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 0 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='input1', partition=7}, KafkaTopicPartition{topic='input1', partition=3}]
2020-02-28 02:14:09,688 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 3 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='input1', partition=6}, KafkaTopicPartition{topic='input1', partition=2}]
2020-02-28 02:14:09,689 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 2 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='input1', partition=5}, KafkaTopicPartition{topic='input1', partition=1}]
2020-02-28 02:14:09,691 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 1 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='input1', partition=4}, KafkaTopicPartition{topic='input1', partition=0}]
2020-02-28 02:14:09,712 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 2 creating fetcher with offsets {KafkaTopicPartition{topic='input1', partition=5}=-915623761775, KafkaTopicPartition{topic='input1', partition=1}=-915623761775}.
2020-02-28 02:14:09,725 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='input1', partition=7}=-915623761775, KafkaTopicPartition{topic='input1', partition=3}=-915623761775}.
2020-02-28 02:14:09,725 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 3 creating fetcher with offsets {KafkaTopicPartition{topic='input1', partition=6}=-915623761775, KafkaTopicPartition{topic='input1', partition=2}=-915623761775}.
2020-02-28 02:14:09,727 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 1 creating fetcher with offsets {KafkaTopicPartition{topic='input1', partition=4}=-915623761775, KafkaTopicPartition{topic='input1', partition=0}=-915623761775}.
2020-02-28 02:14:09,736 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='input1', partition=7}=-915623761775, KafkaTopicPartition{topic='input1', partition=3}=-915623761775}.
2020-02-28 02:14:09,736 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 2 creating fetcher with offsets {KafkaTopicPartition{topic='input1', partition=5}=-915623761775, KafkaTopicPartition{topic='input1', partition=1}=-915623761775}.
2020-02-28 02:14:09,736 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 1 creating fetcher with offsets {KafkaTopicPartition{topic='input1', partition=4}=-915623761775, KafkaTopicPartition{topic='input1', partition=0}=-915623761775}.
2020-02-28 02:14:09,736 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 3 creating fetcher with offsets {KafkaTopicPartition{topic='input1', partition=6}=-915623761775, KafkaTopicPartition{topic='input1', partition=2}=-915623761775}.
2020-02-28 02:14:09,740 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,743 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,744 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,745 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,753 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,753 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849745
2020-02-28 02:14:09,753 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,753 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,754 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849751
2020-02-28 02:14:09,744 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,745 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,756 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Subscribed to partition(s): input1-6, input1-2
2020-02-28 02:14:09,755 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Subscribed to partition(s): input1-7, input1-3
2020-02-28 02:14:09,759 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,762 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,762 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849748
2020-02-28 02:14:09,763 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,764 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,764 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849759
2020-02-28 02:14:09,765 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Subscribed to partition(s): input1-5, input1-1
2020-02-28 02:14:09,752 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,769 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-7
2020-02-28 02:14:09,749 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,770 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,770 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,771 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849770
2020-02-28 02:14:09,749 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:09,778 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,779 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,779 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849778
2020-02-28 02:14:09,776 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Subscribed to partition(s): input1-6, input1-2
2020-02-28 02:14:09,781 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-6
2020-02-28 02:14:09,781 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Subscribed to partition(s): input1-7, input1-3
2020-02-28 02:14:09,781 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-7
2020-02-28 02:14:09,775 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Subscribed to partition(s): input1-4, input1-0
2020-02-28 02:14:09,769 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-6
2020-02-28 02:14:09,769 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-5
2020-02-28 02:14:09,782 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-4
2020-02-28 02:14:09,799 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,800 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,800 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,800 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849800
2020-02-28 02:14:09,800 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,801 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,800 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:09,810 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,807 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:09,811 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:09,811 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848849806
2020-02-28 02:14:09,805 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Subscribed to partition(s): input1-5, input1-1
2020-02-28 02:14:09,811 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-5
2020-02-28 02:14:09,801 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:09,802 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:09,812 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Subscribed to partition(s): input1-4, input1-0
2020-02-28 02:14:09,815 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-4
2020-02-28 02:14:09,812 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:09,819 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,820 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:09,820 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,822 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:09,826 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,827 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:09,830 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Found no committed offset for partition input1-3
2020-02-28 02:14:09,833 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:09,833 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:09,832 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Found no committed offset for partition input1-3
2020-02-28 02:14:09,830 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Found no committed offset for partition input1-1
2020-02-28 02:14:09,830 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Found no committed offset for partition input1-2
2020-02-28 02:14:09,830 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Found no committed offset for partition input1-2
2020-02-28 02:14:09,830 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Found no committed offset for partition input1-1
2020-02-28 02:14:09,851 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Found no committed offset for partition input1-0
2020-02-28 02:14:09,863 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Resetting offset for partition input1-1 to offset 25.
2020-02-28 02:14:09,863 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Resetting offset for partition input1-2 to offset 30.
2020-02-28 02:14:09,863 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Resetting offset for partition input1-3 to offset 26.
2020-02-28 02:14:09,865 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Resetting offset for partition input1-7 to offset 0.
2020-02-28 02:14:09,865 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-3
2020-02-28 02:14:09,866 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Resetting offset for partition input1-6 to offset 0.
2020-02-28 02:14:09,867 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-2
2020-02-28 02:14:09,868 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Resetting offset for partition input1-3 to offset 26.
2020-02-28 02:14:09,868 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Resetting offset for partition input1-7 to offset 0.
2020-02-28 02:14:09,868 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Resetting offset for partition input1-5 to offset 0.
2020-02-28 02:14:09,868 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-1
2020-02-28 02:14:09,868 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Resetting offset for partition input1-0 to offset 25.
2020-02-28 02:14:09,869 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Resetting offset for partition input1-4 to offset 0.
2020-02-28 02:14:09,869 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-0
2020-02-28 02:14:09,869 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Resetting offset for partition input1-2 to offset 0.
2020-02-28 02:14:09,870 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Resetting offset for partition input1-2 to offset 30.
2020-02-28 02:14:09,866 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Resetting offset for partition input1-1 to offset 25.
2020-02-28 02:14:09,872 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Resetting offset for partition input1-5 to offset 0.
2020-02-28 02:14:09,872 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Resetting offset for partition input1-1 to offset 0.
2020-02-28 02:14:09,871 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Resetting offset for partition input1-0 to offset 0.
2020-02-28 02:14:09,870 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Resetting offset for partition input1-6 to offset 0.
2020-02-28 02:14:09,868 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Resetting offset for partition input1-3 to offset 0.
2020-02-28 02:14:09,868 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-3
2020-02-28 02:14:09,874 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-2
2020-02-28 02:14:09,873 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-1
2020-02-28 02:14:09,877 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Resetting offset for partition input1-3 to offset 0.
2020-02-28 02:14:09,878 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Resetting offset for partition input1-2 to offset 0.
2020-02-28 02:14:09,878 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Resetting offset for partition input1-1 to offset 0.
2020-02-28 02:14:09,879 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Found no committed offset for partition input1-0
2020-02-28 02:14:09,894 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Resetting offset for partition input1-0 to offset 25.
2020-02-28 02:14:09,894 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Resetting offset for partition input1-4 to offset 0.
2020-02-28 02:14:09,894 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input1-0
2020-02-28 02:14:09,905 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Resetting offset for partition input1-0 to offset 0.
2020-02-28 02:14:39,489 [TransientBlobCache shutdown hook] INFO  (AbstractBlobCache.java:247) - Shutting down BLOB cache
2020-02-28 02:14:39,490 [TaskExecutorLocalStateStoresManager shutdown hook] INFO  (TaskExecutorLocalStateStoresManager.java:213) - Shutting down TaskExecutorLocalStateStoresManager.
2020-02-28 02:14:39,491 [PermanentBlobCache shutdown hook] INFO  (AbstractBlobCache.java:247) - Shutting down BLOB cache
2020-02-28 02:14:39,497 [FileCache shutdown hook] INFO  (FileCache.java:153) - removed file cache directory /tmp/flink-dist-cache-f894bed0-6a3a-468b-af51-ed6741fdde38
2020-02-28 02:14:39,500 [BlobServer shutdown hook] INFO  (BlobServer.java:340) - Stopped BLOB server at 0.0.0.0:42821
2020-02-28 02:14:39,501 [IOManagerAsync shutdown hook] INFO  (FileChannelManagerImpl.java:112) - FileChannelManager removed spill file directory /tmp/flink-io-e304bda9-634d-40a4-beb1-f5b58b96e4b0
2020-02-28 02:14:44,777 [main] WARN  (FlinkKafkaProducer.java:667) - Property [transaction.timeout.ms] not specified. Setting it to 3600000 ms
2020-02-28 02:14:44,997 [main] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion does not contain a setter for field one
2020-02-28 02:14:44,997 [main] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:45,000 [main] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion does not contain a setter for field one
2020-02-28 02:14:45,000 [main] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:45,420 [main] INFO  (LocalStreamEnvironment.java:108) - Running job on local embedded Flink mini cluster
2020-02-28 02:14:46,010 [main] INFO  (MiniCluster.java:253) - Starting Flink Mini Cluster
2020-02-28 02:14:46,015 [main] INFO  (MiniCluster.java:262) - Starting Metrics Registry
2020-02-28 02:14:46,121 [main] INFO  (MetricRegistryImpl.java:114) - No metrics reporter configured, no metrics will be exposed/reported.
2020-02-28 02:14:46,121 [main] INFO  (MiniCluster.java:266) - Starting RPC Service(s)
2020-02-28 02:14:46,932 [flink-akka.actor.default-dispatcher-2] INFO  (Slf4jLogger.scala:92) - Slf4jLogger started
2020-02-28 02:14:47,132 [main] INFO  (BootstrapTools.java:244) - Trying to start actor system at :0
2020-02-28 02:14:47,197 [flink-metrics-2] INFO  (Slf4jLogger.scala:92) - Slf4jLogger started
2020-02-28 02:14:47,221 [flink-metrics-2] INFO  (MarkerIgnoringBase.java:107) - Starting remoting
2020-02-28 02:14:47,387 [flink-metrics-2] INFO  (MarkerIgnoringBase.java:107) - Remoting started; listening on addresses :[akka.tcp://flink-metrics@127.0.1.1:34581]
2020-02-28 02:14:47,424 [main] INFO  (BootstrapTools.java:256) - Actor system started at akka.tcp://flink-metrics@127.0.1.1:34581
2020-02-28 02:14:47,433 [main] INFO  (AkkaRpcService.java:223) - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/MetricQueryService .
2020-02-28 02:14:47,449 [main] INFO  (MiniCluster.java:397) - Starting high-availability services
2020-02-28 02:14:47,462 [main] INFO  (BlobServer.java:141) - Created BLOB server storage directory /tmp/blobStore-e79bf7d5-8304-4a35-8c64-53184df2f655
2020-02-28 02:14:47,467 [main] INFO  (BlobServer.java:203) - Started BLOB server at 0.0.0.0:39887 - max concurrent requests: 50 - max backlog: 1000
2020-02-28 02:14:47,473 [main] INFO  (AbstractBlobCache.java:107) - Created BLOB cache storage directory /tmp/blobStore-99dbbc62-1da2-4dbb-b49b-506c423e2923
2020-02-28 02:14:47,475 [main] INFO  (AbstractBlobCache.java:107) - Created BLOB cache storage directory /tmp/blobStore-cfd72d93-d318-47d1-8ff6-014bfa45d66b
2020-02-28 02:14:47,475 [main] INFO  (MiniCluster.java:479) - Starting 1 TaskManger(s)
2020-02-28 02:14:47,478 [main] INFO  (TaskManagerRunner.java:351) - Starting TaskManager with ResourceID: 3a46a133-9ff7-4454-b1ad-e2d42b594c3f
2020-02-28 02:14:47,606 [main] INFO  (TaskManagerServices.java:519) - Temporary file directory '/tmp': total 439 GB, usable 336 GB (76.54% usable)
2020-02-28 02:14:47,610 [main] INFO  (FileChannelManagerImpl.java:76) - FileChannelManager uses directory /tmp/flink-io-09607dde-a6d9-48dd-928c-c87965fbe42d for spill files.
2020-02-28 02:14:47,618 [main] INFO  (FileChannelManagerImpl.java:76) - FileChannelManager uses directory /tmp/flink-netty-shuffle-a828b189-2ef1-4f45-bda2-7d87f71911cb for spill files.
2020-02-28 02:14:47,721 [main] INFO  (NetworkBufferPool.java:140) - Allocated 191 MB for network buffer pool (number of memory segments: 6113, bytes per segment: 32768).
2020-02-28 02:14:47,728 [main] INFO  (NettyShuffleEnvironment.java:283) - Starting the network environment and its components.
2020-02-28 02:14:47,730 [main] INFO  (KvStateService.java:89) - Starting the kvState service and its components.
2020-02-28 02:14:47,730 [main] INFO  (TaskManagerServices.java:364) - Limiting managed memory to 0.7 of the currently free heap space (1197 MB), memory will be allocated lazily.
2020-02-28 02:14:47,743 [main] INFO  (TaskManagerConfiguration.java:197) - Messages have a max timeout of 10000 ms
2020-02-28 02:14:47,752 [main] INFO  (AkkaRpcService.java:223) - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/taskmanager_0 .
2020-02-28 02:14:47,765 [flink-akka.actor.default-dispatcher-3] INFO  (JobLeaderService.java:125) - Start job leader service.
2020-02-28 02:14:47,768 [flink-akka.actor.default-dispatcher-3] INFO  (FileCache.java:107) - User file cache uses directory /tmp/flink-dist-cache-39835915-69ba-4e65-9dc8-5da4e99d0aee
2020-02-28 02:14:47,803 [main] INFO  (RestServerEndpoint.java:136) - Starting rest endpoint.
2020-02-28 02:14:47,863 [main] WARN  (WebMonitorUtils.java:87) - Log file environment variable 'log.file' is not set.
2020-02-28 02:14:47,864 [main] WARN  (WebMonitorUtils.java:93) - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'Key: 'web.log.path' , default: null (fallback keys: [{key=jobmanager.web.log.path, isDeprecated=true}])'.
2020-02-28 02:14:47,873 [main] INFO  (DispatcherRestEndpoint.java:113) - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
2020-02-28 02:14:48,047 [main] INFO  (RestServerEndpoint.java:233) - Rest endpoint listening at localhost:36319
2020-02-28 02:14:48,049 [main] INFO  (EmbeddedLeaderService.java:300) - Proposing leadership to contender org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint@4e31c3ec @ http://localhost:36319
2020-02-28 02:14:48,051 [mini-cluster-io-thread-1] INFO  (WebMonitorEndpoint.java:712) - http://localhost:36319 was granted leadership with leaderSessionID=9a1d6dc5-b137-40ef-a834-f63127d58076
2020-02-28 02:14:48,051 [mini-cluster-io-thread-1] INFO  (EmbeddedLeaderService.java:250) - Received confirmation of leadership for leader http://localhost:36319 , session=9a1d6dc5-b137-40ef-a834-f63127d58076
2020-02-28 02:14:48,065 [main] INFO  (AkkaRpcService.java:223) - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/resourcemanager .
2020-02-28 02:14:48,080 [main] INFO  (AkkaRpcService.java:223) - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/dispatcher .
2020-02-28 02:14:48,092 [flink-akka.actor.default-dispatcher-2] INFO  (EmbeddedLeaderService.java:300) - Proposing leadership to contender org.apache.flink.runtime.resourcemanager.StandaloneResourceManager@cdeeb76 @ akka://flink/user/resourcemanager
2020-02-28 02:14:48,093 [flink-akka.actor.default-dispatcher-3] INFO  (EmbeddedLeaderService.java:300) - Proposing leadership to contender org.apache.flink.runtime.dispatcher.StandaloneDispatcher@425933bf @ akka://flink/user/dispatcher
2020-02-28 02:14:48,099 [flink-akka.actor.default-dispatcher-2] INFO  (ResourceManager.java:925) - ResourceManager akka://flink/user/resourcemanager was granted leadership with fencing token b3a8942974d5b6671d23393847bd4648
2020-02-28 02:14:48,103 [flink-akka.actor.default-dispatcher-3] INFO  (Dispatcher.java:884) - Dispatcher akka://flink/user/dispatcher was granted leadership with fencing token 21b90a4d-5419-4c1c-9101-f67ee4f272fa
2020-02-28 02:14:48,103 [flink-akka.actor.default-dispatcher-2] INFO  (SlotManagerImpl.java:219) - Starting the SlotManager.
2020-02-28 02:14:48,105 [main] INFO  (MiniCluster.java:362) - Flink Mini Cluster started successfully
2020-02-28 02:14:48,106 [flink-akka.actor.default-dispatcher-5] INFO  (Dispatcher.java:716) - Recovering all persisted jobs.
2020-02-28 02:14:48,109 [flink-akka.actor.default-dispatcher-5] INFO  (EmbeddedLeaderService.java:250) - Received confirmation of leadership for leader akka://flink/user/dispatcher , session=21b90a4d-5419-4c1c-9101-f67ee4f272fa
2020-02-28 02:14:48,125 [flink-akka.actor.default-dispatcher-2] INFO  (EmbeddedLeaderService.java:250) - Received confirmation of leadership for leader akka://flink/user/resourcemanager , session=1d233938-47bd-4648-b3a8-942974d5b667
2020-02-28 02:14:48,126 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutor.java:991) - Connecting to ResourceManager akka://flink/user/resourcemanager(b3a8942974d5b6671d23393847bd4648).
2020-02-28 02:14:48,132 [flink-akka.actor.default-dispatcher-3] INFO  (RetryingRegistration.java:155) - Resolved ResourceManager address, beginning registration
2020-02-28 02:14:48,132 [flink-akka.actor.default-dispatcher-3] INFO  (RetryingRegistration.java:204) - Registration at ResourceManager attempt 1 (timeout=100ms)
2020-02-28 02:14:48,144 [flink-akka.actor.default-dispatcher-4] INFO  (ResourceManager.java:717) - Registering TaskManager with ResourceID 3a46a133-9ff7-4454-b1ad-e2d42b594c3f (akka://flink/user/taskmanager_0) at ResourceManager
2020-02-28 02:14:48,146 [flink-akka.actor.default-dispatcher-5] INFO  (TaskExecutorToResourceManagerConnection.java:100) - Successful registration at resource manager akka://flink/user/resourcemanager under registration id f7942287816d8dc9314e45fc3d3fc069.
2020-02-28 02:14:48,151 [flink-akka.actor.default-dispatcher-3] INFO  (Dispatcher.java:264) - Received JobGraph submission da29530c357df06091cdc81e6ff07c64 (SecondAlgorithmPass).
2020-02-28 02:14:48,152 [flink-akka.actor.default-dispatcher-3] INFO  (Dispatcher.java:321) - Submitting job da29530c357df06091cdc81e6ff07c64 (SecondAlgorithmPass).
2020-02-28 02:14:48,168 [flink-akka.actor.default-dispatcher-5] INFO  (AkkaRpcService.java:223) - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/jobmanager_1 .
2020-02-28 02:14:48,178 [flink-akka.actor.default-dispatcher-5] INFO  (JobMaster.java:242) - Initializing job SecondAlgorithmPass (da29530c357df06091cdc81e6ff07c64).
2020-02-28 02:14:48,198 [flink-akka.actor.default-dispatcher-5] INFO  (LegacyScheduler.java:171) - Using restart strategy NoRestartStrategy for SecondAlgorithmPass (da29530c357df06091cdc81e6ff07c64).
2020-02-28 02:14:48,214 [flink-akka.actor.default-dispatcher-5] INFO  (ExecutionGraph.java:519) - Job recovers via failover strategy: full graph restart
2020-02-28 02:14:48,227 [flink-akka.actor.default-dispatcher-5] INFO  (ExecutionGraphBuilder.java:204) - Running initialization on master for job SecondAlgorithmPass (da29530c357df06091cdc81e6ff07c64).
2020-02-28 02:14:48,228 [flink-akka.actor.default-dispatcher-5] INFO  (ExecutionGraphBuilder.java:222) - Successfully ran initialization on master in 0 ms.
2020-02-28 02:14:48,269 [flink-akka.actor.default-dispatcher-5] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,280 [flink-akka.actor.default-dispatcher-5] INFO  (EmbeddedLeaderService.java:300) - Proposing leadership to contender org.apache.flink.runtime.jobmaster.JobManagerRunner@7f4b3fed @ akka://flink/user/jobmanager_1
2020-02-28 02:14:48,281 [mini-cluster-io-thread-3] INFO  (JobManagerRunner.java:313) - JobManager runner for job SecondAlgorithmPass (da29530c357df06091cdc81e6ff07c64) was granted leadership with session id b36bca60-5fbe-47c4-a36b-d7d18cde6e33 at akka://flink/user/jobmanager_1.
2020-02-28 02:14:48,287 [flink-akka.actor.default-dispatcher-2] INFO  (JobMaster.java:712) - Starting execution of job SecondAlgorithmPass (da29530c357df06091cdc81e6ff07c64) under job master id a36bd7d18cde6e33b36bca605fbe47c4.
2020-02-28 02:14:48,288 [flink-akka.actor.default-dispatcher-2] INFO  (ExecutionGraph.java:1325) - Job SecondAlgorithmPass (da29530c357df06091cdc81e6ff07c64) switched from state CREATED to RUNNING.
2020-02-28 02:14:48,292 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (cac5e2809c72573fb245cbd2adff17bb) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,305 [flink-akka.actor.default-dispatcher-2] INFO  (SlotPoolImpl.java:369) - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{1d800f918772a6a0b0d06bc4aef2086f}]
2020-02-28 02:14:48,314 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (d107335508fe7f444a36443c505873af) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,314 [flink-akka.actor.default-dispatcher-2] INFO  (SlotPoolImpl.java:369) - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{241f15105654b3d28f417259540c9130}]
2020-02-28 02:14:48,315 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (8d1740b2e73242af4032d403365f0373) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,315 [flink-akka.actor.default-dispatcher-2] INFO  (SlotPoolImpl.java:369) - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{edf4c5cc353f3871dddfbb7b7ceb176d}]
2020-02-28 02:14:48,315 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (0bb599f78393a414564461580d7316bc) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,316 [flink-akka.actor.default-dispatcher-2] INFO  (SlotPoolImpl.java:369) - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{a86d35e4f3a99a767565315bef249c67}]
2020-02-28 02:14:48,316 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (1/4) (6941bda7337eacc9a65ea3263a5b149d) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,316 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (2/4) (330140478377015c832de40202679818) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,317 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (3/4) (65626137ea589861ec703bc005c50fa0) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,317 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (4/4) (41cadee1552a4eff80b56d8aa0b227b8) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,317 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (97eb25963ee78c6d53c73e541f51e87a) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,317 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (a9bc0a60bf708bd21dec8fcf9054da51) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,318 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (3d595d67604528286934bc73cb911295) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,318 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (7c5930c7f36d2978063ff4d4edd76590) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,319 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (1/4) (ecf5cd9620582719590c5a38d284fcf7) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,319 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (2/4) (1eb93e72a0fd2530f7eba65e8b034779) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,320 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (3/4) (c3c363a79c86bc0e6affbcedec19da00) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,321 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (4/4) (1314d0a30bee2256ff13635259fe81e8) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,321 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4) (341c8c775649237f9c43353a5ff05f81) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,322 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4) (790d6259037066553820a917e52bd090) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,323 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4) (035cc223b9b80379728124a3e3812ace) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,323 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4) (b80d30d5d8b46bdce06b215903184668) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,323 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - KeyedProcess (1/4) (bba6491088459f48c4d360f6a62ad87d) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,323 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - KeyedProcess (2/4) (07fc4b9b0062b2d77b1c31febebf9da4) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,324 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - KeyedProcess (3/4) (10293fde235fa77aa388c43a3fb05463) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,324 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - KeyedProcess (4/4) (d159a46941a7907336168d33198487b9) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,324 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Sink: Unnamed (1/4) (dede82d3be89c7ded9fa529ba710d6f4) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,324 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Sink: Unnamed (2/4) (214eaf6d032509dbfc6ebcb87aeb2995) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,325 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Sink: Unnamed (3/4) (b2e0ee5da825625f22b982d63a1379e8) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,325 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Sink: Unnamed (4/4) (85b81f3fffd2ff1d97e1c15d7bdcfd5c) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,325 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Flat Map (1/4) (5ad4617825a086181683ceca42316f2e) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,326 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Flat Map (2/4) (61c79add862fc179249edcf871b1d267) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,326 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Flat Map (3/4) (31d9449db7d4b4a2cec4eaa1b5ccdf08) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,326 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Flat Map (4/4) (d19fa2636b8c6e82bf99c925e9b28e82) switched from CREATED to SCHEDULED.
2020-02-28 02:14:48,328 [jobmanager-future-thread-1] INFO  (EmbeddedLeaderService.java:250) - Received confirmation of leadership for leader akka://flink/user/jobmanager_1 , session=b36bca60-5fbe-47c4-a36b-d7d18cde6e33
2020-02-28 02:14:48,332 [flink-akka.actor.default-dispatcher-2] INFO  (JobMaster.java:936) - Connecting to ResourceManager akka://flink/user/resourcemanager(b3a8942974d5b6671d23393847bd4648)
2020-02-28 02:14:48,334 [flink-akka.actor.default-dispatcher-5] INFO  (RetryingRegistration.java:155) - Resolved ResourceManager address, beginning registration
2020-02-28 02:14:48,334 [flink-akka.actor.default-dispatcher-5] INFO  (RetryingRegistration.java:204) - Registration at ResourceManager attempt 1 (timeout=100ms)
2020-02-28 02:14:48,336 [flink-akka.actor.default-dispatcher-2] INFO  (ResourceManager.java:306) - Registering job manager a36bd7d18cde6e33b36bca605fbe47c4@akka://flink/user/jobmanager_1 for job da29530c357df06091cdc81e6ff07c64.
2020-02-28 02:14:48,344 [flink-akka.actor.default-dispatcher-5] INFO  (ResourceManager.java:661) - Registered job manager a36bd7d18cde6e33b36bca605fbe47c4@akka://flink/user/jobmanager_1 for job da29530c357df06091cdc81e6ff07c64.
2020-02-28 02:14:48,349 [flink-akka.actor.default-dispatcher-2] INFO  (JobMaster.java:958) - JobManager successfully registered at ResourceManager, leader id: b3a8942974d5b6671d23393847bd4648.
2020-02-28 02:14:48,350 [flink-akka.actor.default-dispatcher-2] INFO  (SlotPoolImpl.java:319) - Requesting new slot [SlotRequestId{1d800f918772a6a0b0d06bc4aef2086f}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
2020-02-28 02:14:48,352 [flink-akka.actor.default-dispatcher-5] INFO  (ResourceManager.java:441) - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job da29530c357df06091cdc81e6ff07c64 with allocation id 4b6005a9a8815cad61e0bbcf5071f179.
2020-02-28 02:14:48,357 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:822) - Receive slot request 4b6005a9a8815cad61e0bbcf5071f179 for job da29530c357df06091cdc81e6ff07c64 from resource manager with leader id b3a8942974d5b6671d23393847bd4648.
2020-02-28 02:14:48,357 [flink-akka.actor.default-dispatcher-2] INFO  (SlotPoolImpl.java:319) - Requesting new slot [SlotRequestId{241f15105654b3d28f417259540c9130}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
2020-02-28 02:14:48,357 [flink-akka.actor.default-dispatcher-2] INFO  (SlotPoolImpl.java:319) - Requesting new slot [SlotRequestId{edf4c5cc353f3871dddfbb7b7ceb176d}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
2020-02-28 02:14:48,358 [flink-akka.actor.default-dispatcher-2] INFO  (SlotPoolImpl.java:319) - Requesting new slot [SlotRequestId{a86d35e4f3a99a767565315bef249c67}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
2020-02-28 02:14:48,362 [flink-akka.actor.default-dispatcher-5] INFO  (ResourceManager.java:441) - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job da29530c357df06091cdc81e6ff07c64 with allocation id a6731cd3d9e57c9fa41bcc9fb52e9d65.
2020-02-28 02:14:48,363 [flink-akka.actor.default-dispatcher-5] INFO  (ResourceManager.java:441) - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job da29530c357df06091cdc81e6ff07c64 with allocation id c0ab98802093215ab6ab879354b51a97.
2020-02-28 02:14:48,364 [flink-akka.actor.default-dispatcher-5] INFO  (ResourceManager.java:441) - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job da29530c357df06091cdc81e6ff07c64 with allocation id 64d03db7add7961c598698cd8e592017.
2020-02-28 02:14:48,368 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:834) - Allocated slot for 4b6005a9a8815cad61e0bbcf5071f179.
2020-02-28 02:14:48,368 [flink-akka.actor.default-dispatcher-4] INFO  (JobLeaderService.java:193) - Add job da29530c357df06091cdc81e6ff07c64 for job leader monitoring.
2020-02-28 02:14:48,370 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:822) - Receive slot request a6731cd3d9e57c9fa41bcc9fb52e9d65 for job da29530c357df06091cdc81e6ff07c64 from resource manager with leader id b3a8942974d5b6671d23393847bd4648.
2020-02-28 02:14:48,370 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:834) - Allocated slot for a6731cd3d9e57c9fa41bcc9fb52e9d65.
2020-02-28 02:14:48,371 [flink-akka.actor.default-dispatcher-4] INFO  (JobLeaderService.java:193) - Add job da29530c357df06091cdc81e6ff07c64 for job leader monitoring.
2020-02-28 02:14:48,371 [mini-cluster-io-thread-1] INFO  (JobLeaderService.java:333) - Try to register at job manager akka://flink/user/jobmanager_1 with leader id b36bca60-5fbe-47c4-a36b-d7d18cde6e33.
2020-02-28 02:14:48,371 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:822) - Receive slot request c0ab98802093215ab6ab879354b51a97 for job da29530c357df06091cdc81e6ff07c64 from resource manager with leader id b3a8942974d5b6671d23393847bd4648.
2020-02-28 02:14:48,371 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:834) - Allocated slot for c0ab98802093215ab6ab879354b51a97.
2020-02-28 02:14:48,371 [flink-akka.actor.default-dispatcher-4] INFO  (JobLeaderService.java:193) - Add job da29530c357df06091cdc81e6ff07c64 for job leader monitoring.
2020-02-28 02:14:48,372 [mini-cluster-io-thread-3] INFO  (JobLeaderService.java:333) - Try to register at job manager akka://flink/user/jobmanager_1 with leader id b36bca60-5fbe-47c4-a36b-d7d18cde6e33.
2020-02-28 02:14:48,372 [mini-cluster-io-thread-1] WARN  (EmbeddedLeaderService.java:516) - Error notifying leader listener about new leader
java.lang.IllegalStateException: The RPC connection is already closed
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:195)
	at org.apache.flink.runtime.registration.RegisteredRpcConnection.start(RegisteredRpcConnection.java:90)
	at org.apache.flink.runtime.taskexecutor.JobLeaderService$JobManagerLeaderListener.notifyLeaderAddress(JobLeaderService.java:334)
	at org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService$NotifyOfLeaderCall.run(EmbeddedLeaderService.java:513)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-02-28 02:14:48,373 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:822) - Receive slot request 64d03db7add7961c598698cd8e592017 for job da29530c357df06091cdc81e6ff07c64 from resource manager with leader id b3a8942974d5b6671d23393847bd4648.
2020-02-28 02:14:48,375 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:834) - Allocated slot for 64d03db7add7961c598698cd8e592017.
2020-02-28 02:14:48,373 [mini-cluster-io-thread-4] INFO  (JobLeaderService.java:333) - Try to register at job manager akka://flink/user/jobmanager_1 with leader id b36bca60-5fbe-47c4-a36b-d7d18cde6e33.
2020-02-28 02:14:48,375 [flink-akka.actor.default-dispatcher-3] INFO  (RetryingRegistration.java:155) - Resolved JobManager address, beginning registration
2020-02-28 02:14:48,376 [flink-akka.actor.default-dispatcher-5] INFO  (RetryingRegistration.java:155) - Resolved JobManager address, beginning registration
2020-02-28 02:14:48,377 [flink-akka.actor.default-dispatcher-5] INFO  (RetryingRegistration.java:204) - Registration at JobManager attempt 1 (timeout=100ms)
2020-02-28 02:14:48,376 [flink-akka.actor.default-dispatcher-4] INFO  (JobLeaderService.java:193) - Add job da29530c357df06091cdc81e6ff07c64 for job leader monitoring.
2020-02-28 02:14:48,378 [mini-cluster-io-thread-2] INFO  (JobLeaderService.java:333) - Try to register at job manager akka://flink/user/jobmanager_1 with leader id b36bca60-5fbe-47c4-a36b-d7d18cde6e33.
2020-02-28 02:14:48,381 [flink-akka.actor.default-dispatcher-3] INFO  (RetryingRegistration.java:155) - Resolved JobManager address, beginning registration
2020-02-28 02:14:48,381 [flink-akka.actor.default-dispatcher-3] INFO  (RetryingRegistration.java:204) - Registration at JobManager attempt 1 (timeout=100ms)
2020-02-28 02:14:48,382 [flink-akka.actor.default-dispatcher-4] INFO  (JobLeaderService.java:382) - Successful registration at job manager akka://flink/user/jobmanager_1 for job da29530c357df06091cdc81e6ff07c64.
2020-02-28 02:14:48,383 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:1227) - Establish JobManager connection for job da29530c357df06091cdc81e6ff07c64.
2020-02-28 02:14:48,386 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:1128) - Offer reserved slots to the leader of job da29530c357df06091cdc81e6ff07c64.
2020-02-28 02:14:48,402 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (cac5e2809c72573fb245cbd2adff17bb) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,402 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (1/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,413 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (d107335508fe7f444a36443c505873af) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,413 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (2/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,414 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (8d1740b2e73242af4032d403365f0373) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,414 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (3/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,415 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (0bb599f78393a414564461580d7316bc) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,415 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (4/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,416 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (1/4) (6941bda7337eacc9a65ea3263a5b149d) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,416 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Map (1/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,417 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (2/4) (330140478377015c832de40202679818) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,418 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Map (2/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,419 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (3/4) (65626137ea589861ec703bc005c50fa0) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,419 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Map (3/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,419 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (4/4) (41cadee1552a4eff80b56d8aa0b227b8) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,420 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Map (4/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,420 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (97eb25963ee78c6d53c73e541f51e87a) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,421 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (1/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,424 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (a9bc0a60bf708bd21dec8fcf9054da51) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,424 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (2/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,425 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (3d595d67604528286934bc73cb911295) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,425 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (3/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,425 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (7c5930c7f36d2978063ff4d4edd76590) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,426 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (4/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,426 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (1/4) (ecf5cd9620582719590c5a38d284fcf7) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,428 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Map (1/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,429 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (2/4) (1eb93e72a0fd2530f7eba65e8b034779) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,430 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Map (2/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,431 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (3/4) (c3c363a79c86bc0e6affbcedec19da00) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,431 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Map (3/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,432 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (4/4) (1314d0a30bee2256ff13635259fe81e8) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,433 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Map (4/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,434 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4) (341c8c775649237f9c43353a5ff05f81) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,434 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,437 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4) (790d6259037066553820a917e52bd090) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,437 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,438 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4) (035cc223b9b80379728124a3e3812ace) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,438 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,439 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4) (b80d30d5d8b46bdce06b215903184668) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,440 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,440 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (1/4).
2020-02-28 02:14:48,441 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - KeyedProcess (1/4) (bba6491088459f48c4d360f6a62ad87d) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,441 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying KeyedProcess (1/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,442 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - KeyedProcess (2/4) (07fc4b9b0062b2d77b1c31febebf9da4) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,442 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying KeyedProcess (2/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,443 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - KeyedProcess (3/4) (10293fde235fa77aa388c43a3fb05463) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,443 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying KeyedProcess (3/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,444 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - KeyedProcess (4/4) (d159a46941a7907336168d33198487b9) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,444 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying KeyedProcess (4/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,444 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (cac5e2809c72573fb245cbd2adff17bb) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,447 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (2/4).
2020-02-28 02:14:48,445 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Sink: Unnamed (1/4) (dede82d3be89c7ded9fa529ba710d6f4) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,447 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (1/4) (cac5e2809c72573fb245cbd2adff17bb) [DEPLOYING]
2020-02-28 02:14:48,448 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Sink: Unnamed (1/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,449 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Sink: Unnamed (2/4) (214eaf6d032509dbfc6ebcb87aeb2995) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,449 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Sink: Unnamed (2/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,449 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Sink: Unnamed (3/4) (b2e0ee5da825625f22b982d63a1379e8) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,450 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Sink: Unnamed (3/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,451 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Sink: Unnamed (4/4) (85b81f3fffd2ff1d97e1c15d7bdcfd5c) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,451 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Sink: Unnamed (4/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,453 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (d107335508fe7f444a36443c505873af) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,453 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (2/4) (d107335508fe7f444a36443c505873af) [DEPLOYING]
2020-02-28 02:14:48,454 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Flat Map (1/4) (5ad4617825a086181683ceca42316f2e) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,455 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Flat Map (1/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,456 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Flat Map (2/4) (61c79add862fc179249edcf871b1d267) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,457 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Flat Map (2/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,457 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Flat Map (3/4) (31d9449db7d4b4a2cec4eaa1b5ccdf08) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,457 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Flat Map (3/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,458 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Flat Map (4/4) (d19fa2636b8c6e82bf99c925e9b28e82) switched from SCHEDULED to DEPLOYING.
2020-02-28 02:14:48,459 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:713) - Deploying Flat Map (4/4) (attempt #0) to 3a46a133-9ff7-4454-b1ad-e2d42b594c3f @ localhost (dataPort=-1)
2020-02-28 02:14:48,459 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (3/4).
2020-02-28 02:14:48,465 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (8d1740b2e73242af4032d403365f0373) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,465 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (3/4) (8d1740b2e73242af4032d403365f0373) [DEPLOYING]
2020-02-28 02:14:48,472 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Map (2/4).
2020-02-28 02:14:48,473 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (1/4) (cac5e2809c72573fb245cbd2adff17bb) [DEPLOYING].
2020-02-28 02:14:48,473 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (3/4) (8d1740b2e73242af4032d403365f0373) [DEPLOYING].
2020-02-28 02:14:48,473 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (2/4) (d107335508fe7f444a36443c505873af) [DEPLOYING].
2020-02-28 02:14:48,475 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (1/4) (cac5e2809c72573fb245cbd2adff17bb) [DEPLOYING].
2020-02-28 02:14:48,478 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (3/4) (8d1740b2e73242af4032d403365f0373) [DEPLOYING].
2020-02-28 02:14:48,479 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (2/4) (d107335508fe7f444a36443c505873af) [DEPLOYING].
2020-02-28 02:14:48,482 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (2/4) (330140478377015c832de40202679818) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,483 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Map (2/4) (330140478377015c832de40202679818) [DEPLOYING]
2020-02-28 02:14:48,483 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Map (2/4) (330140478377015c832de40202679818) [DEPLOYING].
2020-02-28 02:14:48,484 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Map (2/4) (330140478377015c832de40202679818) [DEPLOYING].
2020-02-28 02:14:48,494 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (8d1740b2e73242af4032d403365f0373) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,488 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Map (3/4).
2020-02-28 02:14:48,494 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (d107335508fe7f444a36443c505873af) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,496 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,497 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (d107335508fe7f444a36443c505873af) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,498 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (8d1740b2e73242af4032d403365f0373) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,501 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (cac5e2809c72573fb245cbd2adff17bb) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,502 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (3/4) (65626137ea589861ec703bc005c50fa0) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,502 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (cac5e2809c72573fb245cbd2adff17bb) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,502 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Map (3/4) (65626137ea589861ec703bc005c50fa0) [DEPLOYING]
2020-02-28 02:14:48,502 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Map (3/4) (65626137ea589861ec703bc005c50fa0) [DEPLOYING].
2020-02-28 02:14:48,503 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Map (3/4) (65626137ea589861ec703bc005c50fa0) [DEPLOYING].
2020-02-28 02:14:48,505 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Map (4/4).
2020-02-28 02:14:48,508 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (4/4).
2020-02-28 02:14:48,501 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,510 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (4/4) (41cadee1552a4eff80b56d8aa0b227b8) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,501 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,510 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Map (4/4) (41cadee1552a4eff80b56d8aa0b227b8) [DEPLOYING]
2020-02-28 02:14:48,510 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Map (4/4) (41cadee1552a4eff80b56d8aa0b227b8) [DEPLOYING].
2020-02-28 02:14:48,511 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Map (4/4) (41cadee1552a4eff80b56d8aa0b227b8) [DEPLOYING].
2020-02-28 02:14:48,520 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (0bb599f78393a414564461580d7316bc) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,521 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (1/4).
2020-02-28 02:14:48,530 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (4/4) (0bb599f78393a414564461580d7316bc) [DEPLOYING]
2020-02-28 02:14:48,530 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (4/4) (0bb599f78393a414564461580d7316bc) [DEPLOYING].
2020-02-28 02:14:48,531 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (4/4) (0bb599f78393a414564461580d7316bc) [DEPLOYING].
2020-02-28 02:14:48,534 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (97eb25963ee78c6d53c73e541f51e87a) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,534 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (1/4) (97eb25963ee78c6d53c73e541f51e87a) [DEPLOYING]
2020-02-28 02:14:48,535 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (1/4) (97eb25963ee78c6d53c73e541f51e87a) [DEPLOYING].
2020-02-28 02:14:48,535 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (1/4) (97eb25963ee78c6d53c73e541f51e87a) [DEPLOYING].
2020-02-28 02:14:48,536 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (3/4) (65626137ea589861ec703bc005c50fa0) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,537 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,537 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (3/4) (65626137ea589861ec703bc005c50fa0) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,546 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (97eb25963ee78c6d53c73e541f51e87a) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,546 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,546 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (2/4).
2020-02-28 02:14:48,549 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (97eb25963ee78c6d53c73e541f51e87a) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,546 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (0bb599f78393a414564461580d7316bc) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,549 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (0bb599f78393a414564461580d7316bc) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,549 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,552 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (4/4) (41cadee1552a4eff80b56d8aa0b227b8) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,553 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,553 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (4/4) (41cadee1552a4eff80b56d8aa0b227b8) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,564 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (2/4) (330140478377015c832de40202679818) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,565 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,565 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (2/4) (330140478377015c832de40202679818) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,567 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (3/4).
2020-02-28 02:14:48,568 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (a9bc0a60bf708bd21dec8fcf9054da51) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,568 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (2/4) (a9bc0a60bf708bd21dec8fcf9054da51) [DEPLOYING]
2020-02-28 02:14:48,568 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (2/4) (a9bc0a60bf708bd21dec8fcf9054da51) [DEPLOYING].
2020-02-28 02:14:48,576 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (2/4) (a9bc0a60bf708bd21dec8fcf9054da51) [DEPLOYING].
2020-02-28 02:14:48,577 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (a9bc0a60bf708bd21dec8fcf9054da51) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,578 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (a9bc0a60bf708bd21dec8fcf9054da51) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,578 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,580 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (3d595d67604528286934bc73cb911295) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,580 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (3/4) (3d595d67604528286934bc73cb911295) [DEPLOYING]
2020-02-28 02:14:48,581 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (3/4) (3d595d67604528286934bc73cb911295) [DEPLOYING].
2020-02-28 02:14:48,581 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (3/4) (3d595d67604528286934bc73cb911295) [DEPLOYING].
2020-02-28 02:14:48,582 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (3d595d67604528286934bc73cb911295) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,583 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (3d595d67604528286934bc73cb911295) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,584 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Map (1/4).
2020-02-28 02:14:48,588 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,592 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (1/4) (6941bda7337eacc9a65ea3263a5b149d) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,593 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Map (1/4) (6941bda7337eacc9a65ea3263a5b149d) [DEPLOYING]
2020-02-28 02:14:48,593 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Map (1/4) (6941bda7337eacc9a65ea3263a5b149d) [DEPLOYING].
2020-02-28 02:14:48,593 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Map (1/4) (6941bda7337eacc9a65ea3263a5b149d) [DEPLOYING].
2020-02-28 02:14:48,594 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (4/4).
2020-02-28 02:14:48,597 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Map (1/4).
2020-02-28 02:14:48,604 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (7c5930c7f36d2978063ff4d4edd76590) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,605 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (4/4) (7c5930c7f36d2978063ff4d4edd76590) [DEPLOYING]
2020-02-28 02:14:48,605 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (4/4) (7c5930c7f36d2978063ff4d4edd76590) [DEPLOYING].
2020-02-28 02:14:48,606 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (4/4) (7c5930c7f36d2978063ff4d4edd76590) [DEPLOYING].
2020-02-28 02:14:48,609 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Map (2/4).
2020-02-28 02:14:48,617 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,617 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,625 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Map (3/4).
2020-02-28 02:14:48,623 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,623 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,626 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,626 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,629 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (7c5930c7f36d2978063ff4d4edd76590) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,630 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (7c5930c7f36d2978063ff4d4edd76590) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,628 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,627 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,627 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,631 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,627 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (2/4) (1eb93e72a0fd2530f7eba65e8b034779) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,626 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,632 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,626 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,632 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,632 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (1/4) (ecf5cd9620582719590c5a38d284fcf7) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,632 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Map (2/4) (1eb93e72a0fd2530f7eba65e8b034779) [DEPLOYING]
2020-02-28 02:14:48,633 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Map (2/4) (1eb93e72a0fd2530f7eba65e8b034779) [DEPLOYING].
2020-02-28 02:14:48,633 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Map (2/4) (1eb93e72a0fd2530f7eba65e8b034779) [DEPLOYING].
2020-02-28 02:14:48,626 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (3/4) (c3c363a79c86bc0e6affbcedec19da00) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,626 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,637 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,637 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Map (3/4) (c3c363a79c86bc0e6affbcedec19da00) [DEPLOYING]
2020-02-28 02:14:48,637 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Map (3/4) (c3c363a79c86bc0e6affbcedec19da00) [DEPLOYING].
2020-02-28 02:14:48,637 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (1/4) (6941bda7337eacc9a65ea3263a5b149d) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,638 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,632 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Map (1/4) (ecf5cd9620582719590c5a38d284fcf7) [DEPLOYING]
2020-02-28 02:14:48,640 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Map (1/4) (ecf5cd9620582719590c5a38d284fcf7) [DEPLOYING].
2020-02-28 02:14:48,631 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,631 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,630 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,641 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Map (1/4) (ecf5cd9620582719590c5a38d284fcf7) [DEPLOYING].
2020-02-28 02:14:48,639 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Map (4/4).
2020-02-28 02:14:48,639 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,642 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,638 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (1/4) (6941bda7337eacc9a65ea3263a5b149d) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,638 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Map (3/4) (c3c363a79c86bc0e6affbcedec19da00) [DEPLOYING].
2020-02-28 02:14:48,642 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (2/4) (1eb93e72a0fd2530f7eba65e8b034779) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,643 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,643 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (2/4) (1eb93e72a0fd2530f7eba65e8b034779) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,649 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,650 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,650 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,650 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,653 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (4/4) (1314d0a30bee2256ff13635259fe81e8) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,653 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Map (4/4) (1314d0a30bee2256ff13635259fe81e8) [DEPLOYING]
2020-02-28 02:14:48,653 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Map (4/4) (1314d0a30bee2256ff13635259fe81e8) [DEPLOYING].
2020-02-28 02:14:48,654 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Map (4/4) (1314d0a30bee2256ff13635259fe81e8) [DEPLOYING].
2020-02-28 02:14:48,655 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (1/4) (ecf5cd9620582719590c5a38d284fcf7) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,655 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (1/4) (ecf5cd9620582719590c5a38d284fcf7) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,666 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4).
2020-02-28 02:14:48,667 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,667 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,664 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (3/4) (c3c363a79c86bc0e6affbcedec19da00) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,657 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Map (4/4) (1314d0a30bee2256ff13635259fe81e8) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,655 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,669 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,669 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (4/4) (1314d0a30bee2256ff13635259fe81e8) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,671 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,671 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Map (3/4) (c3c363a79c86bc0e6affbcedec19da00) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,674 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4).
2020-02-28 02:14:48,694 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,695 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,696 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4).
2020-02-28 02:14:48,696 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4) (341c8c775649237f9c43353a5ff05f81) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,696 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4) (341c8c775649237f9c43353a5ff05f81) [DEPLOYING]
2020-02-28 02:14:48,697 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4) (341c8c775649237f9c43353a5ff05f81) [DEPLOYING].
2020-02-28 02:14:48,697 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4) (341c8c775649237f9c43353a5ff05f81) [DEPLOYING].
2020-02-28 02:14:48,697 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,697 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,697 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4) (790d6259037066553820a917e52bd090) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,698 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4) (790d6259037066553820a917e52bd090) [DEPLOYING]
2020-02-28 02:14:48,698 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4) (790d6259037066553820a917e52bd090) [DEPLOYING].
2020-02-28 02:14:48,698 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4) (790d6259037066553820a917e52bd090) [DEPLOYING].
2020-02-28 02:14:48,702 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4) (035cc223b9b80379728124a3e3812ace) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,702 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4) (035cc223b9b80379728124a3e3812ace) [DEPLOYING]
2020-02-28 02:14:48,702 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4).
2020-02-28 02:14:48,703 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-28 02:14:48,705 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,705 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4) (035cc223b9b80379728124a3e3812ace) [DEPLOYING].
2020-02-28 02:14:48,706 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4) (035cc223b9b80379728124a3e3812ace) [DEPLOYING].
2020-02-28 02:14:48,710 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 1 has no restore state.
2020-02-28 02:14:48,711 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task KeyedProcess (1/4).
2020-02-28 02:14:48,712 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 0 has no restore state.
2020-02-28 02:14:48,718 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4) (b80d30d5d8b46bdce06b215903184668) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,720 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 1 has no restore state.
2020-02-28 02:14:48,721 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 3 has no restore state.
2020-02-28 02:14:48,721 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 0 has no restore state.
2020-02-28 02:14:48,721 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 3 has no restore state.
2020-02-28 02:14:48,721 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 1 has no restore state.
2020-02-28 02:14:48,721 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 2 has no restore state.
2020-02-28 02:14:48,720 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 2 has no restore state.
2020-02-28 02:14:48,722 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 2 has no restore state.
2020-02-28 02:14:48,722 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 0 has no restore state.
2020-02-28 02:14:48,722 [KeyedProcess (1/4)] INFO  (Task.java:958) - KeyedProcess (1/4) (bba6491088459f48c4d360f6a62ad87d) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,722 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 0 has no restore state.
2020-02-28 02:14:48,722 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 2 has no restore state.
2020-02-28 02:14:48,721 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 3 has no restore state.
2020-02-28 02:14:48,721 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 3 has no restore state.
2020-02-28 02:14:48,721 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 1 has no restore state.
2020-02-28 02:14:48,722 [KeyedProcess (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task KeyedProcess (1/4) (bba6491088459f48c4d360f6a62ad87d) [DEPLOYING]
2020-02-28 02:14:48,723 [KeyedProcess (1/4)] INFO  (Task.java:593) - Loading JAR files for task KeyedProcess (1/4) (bba6491088459f48c4d360f6a62ad87d) [DEPLOYING].
2020-02-28 02:14:48,723 [KeyedProcess (1/4)] INFO  (Task.java:619) - Registering task at network: KeyedProcess (1/4) (bba6491088459f48c4d360f6a62ad87d) [DEPLOYING].
2020-02-28 02:14:48,724 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4) (b80d30d5d8b46bdce06b215903184668) [DEPLOYING]
2020-02-28 02:14:48,726 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task KeyedProcess (2/4).
2020-02-28 02:14:48,728 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4) (b80d30d5d8b46bdce06b215903184668) [DEPLOYING].
2020-02-28 02:14:48,731 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4) (b80d30d5d8b46bdce06b215903184668) [DEPLOYING].
2020-02-28 02:14:48,741 [KeyedProcess (2/4)] INFO  (Task.java:958) - KeyedProcess (2/4) (07fc4b9b0062b2d77b1c31febebf9da4) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,741 [KeyedProcess (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task KeyedProcess (2/4) (07fc4b9b0062b2d77b1c31febebf9da4) [DEPLOYING]
2020-02-28 02:14:48,741 [KeyedProcess (2/4)] INFO  (Task.java:593) - Loading JAR files for task KeyedProcess (2/4) (07fc4b9b0062b2d77b1c31febebf9da4) [DEPLOYING].
2020-02-28 02:14:48,745 [KeyedProcess (2/4)] INFO  (Task.java:619) - Registering task at network: KeyedProcess (2/4) (07fc4b9b0062b2d77b1c31febebf9da4) [DEPLOYING].
2020-02-28 02:14:48,746 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task KeyedProcess (3/4).
2020-02-28 02:14:48,750 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task KeyedProcess (4/4).
2020-02-28 02:14:48,752 [KeyedProcess (3/4)] INFO  (Task.java:958) - KeyedProcess (3/4) (10293fde235fa77aa388c43a3fb05463) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,753 [KeyedProcess (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task KeyedProcess (3/4) (10293fde235fa77aa388c43a3fb05463) [DEPLOYING]
2020-02-28 02:14:48,753 [KeyedProcess (3/4)] INFO  (Task.java:593) - Loading JAR files for task KeyedProcess (3/4) (10293fde235fa77aa388c43a3fb05463) [DEPLOYING].
2020-02-28 02:14:48,754 [KeyedProcess (3/4)] INFO  (Task.java:619) - Registering task at network: KeyedProcess (3/4) (10293fde235fa77aa388c43a3fb05463) [DEPLOYING].
2020-02-28 02:14:48,756 [KeyedProcess (4/4)] INFO  (Task.java:958) - KeyedProcess (4/4) (d159a46941a7907336168d33198487b9) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,756 [KeyedProcess (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task KeyedProcess (4/4) (d159a46941a7907336168d33198487b9) [DEPLOYING]
2020-02-28 02:14:48,757 [KeyedProcess (4/4)] INFO  (Task.java:593) - Loading JAR files for task KeyedProcess (4/4) (d159a46941a7907336168d33198487b9) [DEPLOYING].
2020-02-28 02:14:48,758 [KeyedProcess (4/4)] INFO  (Task.java:619) - Registering task at network: KeyedProcess (4/4) (d159a46941a7907336168d33198487b9) [DEPLOYING].
2020-02-28 02:14:48,759 [KeyedProcess (1/4)] INFO  (Task.java:958) - KeyedProcess (1/4) (bba6491088459f48c4d360f6a62ad87d) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,759 [KeyedProcess (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,763 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - KeyedProcess (1/4) (bba6491088459f48c4d360f6a62ad87d) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,761 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Sink: Unnamed (1/4).
2020-02-28 02:14:48,770 [Sink: Unnamed (1/4)] INFO  (Task.java:958) - Sink: Unnamed (1/4) (dede82d3be89c7ded9fa529ba710d6f4) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,771 [Sink: Unnamed (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Sink: Unnamed (1/4) (dede82d3be89c7ded9fa529ba710d6f4) [DEPLOYING]
2020-02-28 02:14:48,771 [Sink: Unnamed (1/4)] INFO  (Task.java:593) - Loading JAR files for task Sink: Unnamed (1/4) (dede82d3be89c7ded9fa529ba710d6f4) [DEPLOYING].
2020-02-28 02:14:48,772 [Sink: Unnamed (1/4)] INFO  (Task.java:619) - Registering task at network: Sink: Unnamed (1/4) (dede82d3be89c7ded9fa529ba710d6f4) [DEPLOYING].
2020-02-28 02:14:48,773 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4) (790d6259037066553820a917e52bd090) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,774 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4) (790d6259037066553820a917e52bd090) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,774 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,779 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,779 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,780 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,780 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,779 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,780 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,780 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,829 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4) (035cc223b9b80379728124a3e3812ace) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,825 [KeyedProcess (3/4)] INFO  (Task.java:958) - KeyedProcess (3/4) (10293fde235fa77aa388c43a3fb05463) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,846 [KeyedProcess (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,820 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4) (341c8c775649237f9c43353a5ff05f81) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,817 [Sink: Unnamed (1/4)] INFO  (Task.java:958) - Sink: Unnamed (1/4) (dede82d3be89c7ded9fa529ba710d6f4) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,786 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,786 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,786 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,786 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,786 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,785 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,785 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,785 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,785 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:48,785 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Sink: Unnamed (2/4).
2020-02-28 02:14:48,862 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Sink: Unnamed (3/4).
2020-02-28 02:14:48,863 [Sink: Unnamed (2/4)] INFO  (Task.java:958) - Sink: Unnamed (2/4) (214eaf6d032509dbfc6ebcb87aeb2995) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,849 [KeyedProcess (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,852 [KeyedProcess (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,849 [Sink: Unnamed (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,849 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,849 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,845 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4) (035cc223b9b80379728124a3e3812ace) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,872 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - KeyedProcess (3/4) (10293fde235fa77aa388c43a3fb05463) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,872 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4) (341c8c775649237f9c43353a5ff05f81) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,844 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,886 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,886 [KeyedProcess (3/4)] INFO  (TypeExtractor.java:1815) - class java.util.Vector does not contain a getter for field elementData
2020-02-28 02:14:48,844 [KeyedProcess (4/4)] INFO  (Task.java:958) - KeyedProcess (4/4) (d159a46941a7907336168d33198487b9) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,841 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4) (b80d30d5d8b46bdce06b215903184668) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,887 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,889 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,832 [KeyedProcess (2/4)] INFO  (Task.java:958) - KeyedProcess (2/4) (07fc4b9b0062b2d77b1c31febebf9da4) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,887 [KeyedProcess (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,889 [KeyedProcess (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,891 [KeyedProcess (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,891 [KeyedProcess (4/4)] INFO  (TypeExtractor.java:1815) - class java.util.Vector does not contain a getter for field elementData
2020-02-28 02:14:48,891 [KeyedProcess (4/4)] INFO  (TypeExtractor.java:1818) - class java.util.Vector does not contain a setter for field elementData
2020-02-28 02:14:48,892 [KeyedProcess (4/4)] INFO  (TypeExtractor.java:1857) - Class class java.util.Vector cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,887 [KeyedProcess (3/4)] INFO  (TypeExtractor.java:1818) - class java.util.Vector does not contain a setter for field elementData
2020-02-28 02:14:48,886 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Sink: Unnamed (1/4) (dede82d3be89c7ded9fa529ba710d6f4) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,894 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - KeyedProcess (4/4) (d159a46941a7907336168d33198487b9) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,894 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (4/4) (b80d30d5d8b46bdce06b215903184668) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,894 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - KeyedProcess (2/4) (07fc4b9b0062b2d77b1c31febebf9da4) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,876 [Sink: Unnamed (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,875 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,874 [Sink: Unnamed (3/4)] INFO  (Task.java:958) - Sink: Unnamed (3/4) (b2e0ee5da825625f22b982d63a1379e8) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,896 [Sink: Unnamed (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Sink: Unnamed (3/4) (b2e0ee5da825625f22b982d63a1379e8) [DEPLOYING]
2020-02-28 02:14:48,896 [KeyedProcess (1/4)] INFO  (TypeExtractor.java:1815) - class java.util.Vector does not contain a getter for field elementData
2020-02-28 02:14:48,896 [KeyedProcess (1/4)] INFO  (TypeExtractor.java:1818) - class java.util.Vector does not contain a setter for field elementData
2020-02-28 02:14:48,897 [KeyedProcess (1/4)] INFO  (TypeExtractor.java:1857) - Class class java.util.Vector cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,870 [Sink: Unnamed (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Sink: Unnamed (2/4) (214eaf6d032509dbfc6ebcb87aeb2995) [DEPLOYING]
2020-02-28 02:14:48,865 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Sink: Unnamed (4/4).
2020-02-28 02:14:48,896 [Sink: Unnamed (3/4)] INFO  (Task.java:593) - Loading JAR files for task Sink: Unnamed (3/4) (b2e0ee5da825625f22b982d63a1379e8) [DEPLOYING].
2020-02-28 02:14:48,898 [Sink: Unnamed (3/4)] INFO  (Task.java:619) - Registering task at network: Sink: Unnamed (3/4) (b2e0ee5da825625f22b982d63a1379e8) [DEPLOYING].
2020-02-28 02:14:48,898 [Sink: Unnamed (3/4)] INFO  (Task.java:958) - Sink: Unnamed (3/4) (b2e0ee5da825625f22b982d63a1379e8) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,895 [Sink: Unnamed (1/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:48,893 [KeyedProcess (3/4)] INFO  (TypeExtractor.java:1857) - Class class java.util.Vector cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,891 [KeyedProcess (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,899 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Sink: Unnamed (3/4) (b2e0ee5da825625f22b982d63a1379e8) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,898 [Sink: Unnamed (2/4)] INFO  (Task.java:593) - Loading JAR files for task Sink: Unnamed (2/4) (214eaf6d032509dbfc6ebcb87aeb2995) [DEPLOYING].
2020-02-28 02:14:48,900 [KeyedProcess (2/4)] INFO  (TypeExtractor.java:1815) - class java.util.Vector does not contain a getter for field elementData
2020-02-28 02:14:48,901 [KeyedProcess (2/4)] INFO  (TypeExtractor.java:1818) - class java.util.Vector does not contain a setter for field elementData
2020-02-28 02:14:48,901 [KeyedProcess (2/4)] INFO  (TypeExtractor.java:1857) - Class class java.util.Vector cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-28 02:14:48,903 [Sink: Unnamed (2/4)] INFO  (Task.java:619) - Registering task at network: Sink: Unnamed (2/4) (214eaf6d032509dbfc6ebcb87aeb2995) [DEPLOYING].
2020-02-28 02:14:48,904 [Sink: Unnamed (2/4)] INFO  (Task.java:958) - Sink: Unnamed (2/4) (214eaf6d032509dbfc6ebcb87aeb2995) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,899 [Sink: Unnamed (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,899 [Sink: Unnamed (1/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 0/4 - no state to restore
2020-02-28 02:14:48,906 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Flat Map (1/4).
2020-02-28 02:14:48,906 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Sink: Unnamed (2/4) (214eaf6d032509dbfc6ebcb87aeb2995) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,907 [Sink: Unnamed (4/4)] INFO  (Task.java:958) - Sink: Unnamed (4/4) (85b81f3fffd2ff1d97e1c15d7bdcfd5c) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,907 [Sink: Unnamed (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Sink: Unnamed (4/4) (85b81f3fffd2ff1d97e1c15d7bdcfd5c) [DEPLOYING]
2020-02-28 02:14:48,907 [Sink: Unnamed (4/4)] INFO  (Task.java:593) - Loading JAR files for task Sink: Unnamed (4/4) (85b81f3fffd2ff1d97e1c15d7bdcfd5c) [DEPLOYING].
2020-02-28 02:14:48,908 [Sink: Unnamed (4/4)] INFO  (Task.java:619) - Registering task at network: Sink: Unnamed (4/4) (85b81f3fffd2ff1d97e1c15d7bdcfd5c) [DEPLOYING].
2020-02-28 02:14:48,908 [Sink: Unnamed (4/4)] INFO  (Task.java:958) - Sink: Unnamed (4/4) (85b81f3fffd2ff1d97e1c15d7bdcfd5c) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,908 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Sink: Unnamed (4/4) (85b81f3fffd2ff1d97e1c15d7bdcfd5c) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,909 [Sink: Unnamed (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,909 [Sink: Unnamed (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,910 [Sink: Unnamed (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,922 [Sink: Unnamed (3/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:48,922 [Sink: Unnamed (1/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:48,923 [Flat Map (1/4)] INFO  (Task.java:958) - Flat Map (1/4) (5ad4617825a086181683ceca42316f2e) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,925 [Flat Map (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (1/4) (5ad4617825a086181683ceca42316f2e) [DEPLOYING]
2020-02-28 02:14:48,926 [Flat Map (1/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (1/4) (5ad4617825a086181683ceca42316f2e) [DEPLOYING].
2020-02-28 02:14:48,927 [Flat Map (1/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (1/4) (5ad4617825a086181683ceca42316f2e) [DEPLOYING].
2020-02-28 02:14:48,928 [Flat Map (1/4)] INFO  (Task.java:958) - Flat Map (1/4) (5ad4617825a086181683ceca42316f2e) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,928 [Flat Map (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,929 [Sink: Unnamed (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,930 [Sink: Unnamed (4/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:48,930 [Sink: Unnamed (4/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 3/4 - no state to restore
2020-02-28 02:14:48,921 [Sink: Unnamed (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,921 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Flat Map (2/4).
2020-02-28 02:14:48,928 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (1/4) (5ad4617825a086181683ceca42316f2e) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,923 [Sink: Unnamed (3/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 2/4 - no state to restore
2020-02-28 02:14:48,931 [Sink: Unnamed (2/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-28 02:14:48,932 [Sink: Unnamed (2/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 1/4 - no state to restore
2020-02-28 02:14:48,933 [Flat Map (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,932 [Sink: Unnamed (3/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:48,941 [Flat Map (2/4)] INFO  (Task.java:958) - Flat Map (2/4) (61c79add862fc179249edcf871b1d267) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,941 [Flat Map (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (2/4) (61c79add862fc179249edcf871b1d267) [DEPLOYING]
2020-02-28 02:14:48,941 [Flat Map (2/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (2/4) (61c79add862fc179249edcf871b1d267) [DEPLOYING].
2020-02-28 02:14:48,941 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Flat Map (3/4).
2020-02-28 02:14:48,931 [Sink: Unnamed (4/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:48,932 [Sink: Unnamed (2/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-28 02:14:48,945 [Flat Map (2/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (2/4) (61c79add862fc179249edcf871b1d267) [DEPLOYING].
2020-02-28 02:14:48,955 [Flat Map (2/4)] INFO  (Task.java:958) - Flat Map (2/4) (61c79add862fc179249edcf871b1d267) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,955 [Flat Map (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,953 [Flat Map (3/4)] INFO  (Task.java:958) - Flat Map (3/4) (31d9449db7d4b4a2cec4eaa1b5ccdf08) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,953 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Flat Map (4/4).
2020-02-28 02:14:48,956 [Flat Map (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,956 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (2/4) (61c79add862fc179249edcf871b1d267) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,960 [Flat Map (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (3/4) (31d9449db7d4b4a2cec4eaa1b5ccdf08) [DEPLOYING]
2020-02-28 02:14:48,961 [flink-akka.actor.default-dispatcher-4] INFO  (TaskSlotTable.java:242) - Activate slot c0ab98802093215ab6ab879354b51a97.
2020-02-28 02:14:48,961 [Flat Map (4/4)] INFO  (Task.java:958) - Flat Map (4/4) (d19fa2636b8c6e82bf99c925e9b28e82) switched from CREATED to DEPLOYING.
2020-02-28 02:14:48,961 [Flat Map (3/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (3/4) (31d9449db7d4b4a2cec4eaa1b5ccdf08) [DEPLOYING].
2020-02-28 02:14:48,962 [Flat Map (3/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (3/4) (31d9449db7d4b4a2cec4eaa1b5ccdf08) [DEPLOYING].
2020-02-28 02:14:48,962 [Flat Map (3/4)] INFO  (Task.java:958) - Flat Map (3/4) (31d9449db7d4b4a2cec4eaa1b5ccdf08) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,963 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Flat Map (3/4) (31d9449db7d4b4a2cec4eaa1b5ccdf08) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,963 [Flat Map (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,961 [flink-akka.actor.default-dispatcher-4] INFO  (TaskSlotTable.java:242) - Activate slot 64d03db7add7961c598698cd8e592017.
2020-02-28 02:14:48,965 [flink-akka.actor.default-dispatcher-4] INFO  (TaskSlotTable.java:242) - Activate slot 4b6005a9a8815cad61e0bbcf5071f179.
2020-02-28 02:14:48,966 [flink-akka.actor.default-dispatcher-4] INFO  (TaskSlotTable.java:242) - Activate slot a6731cd3d9e57c9fa41bcc9fb52e9d65.
2020-02-28 02:14:48,968 [Flat Map (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (4/4) (d19fa2636b8c6e82bf99c925e9b28e82) [DEPLOYING]
2020-02-28 02:14:48,968 [Flat Map (4/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (4/4) (d19fa2636b8c6e82bf99c925e9b28e82) [DEPLOYING].
2020-02-28 02:14:48,969 [Flat Map (4/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (4/4) (d19fa2636b8c6e82bf99c925e9b28e82) [DEPLOYING].
2020-02-28 02:14:48,969 [Flat Map (4/4)] INFO  (Task.java:958) - Flat Map (4/4) (d19fa2636b8c6e82bf99c925e9b28e82) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,969 [Flat Map (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:48,969 [Flat Map (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-28 02:14:48,969 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Flat Map (4/4) (d19fa2636b8c6e82bf99c925e9b28e82) switched from DEPLOYING to RUNNING.
2020-02-28 02:14:48,982 [Flat Map (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-28 02:14:49,036 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,037 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,037 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889035
2020-02-28 02:14:49,039 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,040 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,040 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889038
2020-02-28 02:14:49,064 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,064 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,064 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889037
2020-02-28 02:14:49,076 [Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,078 [Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,079 [Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889076
2020-02-28 02:14:49,091 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,091 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,091 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889068
2020-02-28 02:14:49,091 [Sink: Unnamed (1/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (1/4) to produce into default topic output2
2020-02-28 02:14:49,092 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,092 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,092 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889064
2020-02-28 02:14:49,092 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,092 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,092 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889060
2020-02-28 02:14:49,093 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,093 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,093 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889054
2020-02-28 02:14:49,093 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,093 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,093 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889052
2020-02-28 02:14:49,094 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,094 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,094 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889052
2020-02-28 02:14:49,094 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,094 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,094 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889050
2020-02-28 02:14:49,094 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,095 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,095 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889048
2020-02-28 02:14:49,095 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,095 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,095 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889047
2020-02-28 02:14:49,095 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,095 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,096 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889045
2020-02-28 02:14:49,096 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,096 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,096 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889043
2020-02-28 02:14:49,096 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,096 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,096 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889092
2020-02-28 02:14:49,097 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,097 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,097 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889091
2020-02-28 02:14:49,097 [Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,097 [Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,098 [Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889076
2020-02-28 02:14:49,098 [Sink: Unnamed (2/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (2/4) to produce into default topic output2
2020-02-28 02:14:49,098 [Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,099 [Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,099 [Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889076
2020-02-28 02:14:49,099 [Sink: Unnamed (4/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (4/4) to produce into default topic output2
2020-02-28 02:14:49,100 [Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,100 [Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,100 [Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889096
2020-02-28 02:14:49,100 [Sink: Unnamed (3/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (3/4) to produce into default topic output2
2020-02-28 02:14:49,435 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-2, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,435 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,452 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,452 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,444 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-7, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,444 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,444 [kafka-producer-network-thread | producer-4] INFO  (Metadata.java:261) - [Producer clientId=producer-4] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,443 [kafka-producer-network-thread | producer-2] INFO  (Metadata.java:261) - [Producer clientId=producer-2] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,442 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-8, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,442 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,442 [kafka-producer-network-thread | producer-3] INFO  (Metadata.java:261) - [Producer clientId=producer-3] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,458 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 2 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='_input1', partition=4}, KafkaTopicPartition{topic='_input1', partition=0}]
2020-02-28 02:14:49,442 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-5, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,458 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 3 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='_input1', partition=5}, KafkaTopicPartition{topic='_input1', partition=1}]
2020-02-28 02:14:49,458 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 2 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='output', partition=3}, KafkaTopicPartition{topic='output', partition=7}]
2020-02-28 02:14:49,442 [kafka-producer-network-thread | producer-1] INFO  (Metadata.java:261) - [Producer clientId=producer-1] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,442 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,459 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 1 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='output', partition=2}, KafkaTopicPartition{topic='output', partition=6}]
2020-02-28 02:14:49,441 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-1, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,460 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 2 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='output', partition=3}, KafkaTopicPartition{topic='output', partition=7}]
2020-02-28 02:14:49,460 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 0 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='output', partition=1}, KafkaTopicPartition{topic='output', partition=5}]
2020-02-28 02:14:49,440 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,440 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,461 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 0 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='_input1', partition=6}, KafkaTopicPartition{topic='_input1', partition=2}]
2020-02-28 02:14:49,439 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-4, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,438 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-3, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,463 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 1 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='output', partition=2}, KafkaTopicPartition{topic='output', partition=6}]
2020-02-28 02:14:49,437 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-6, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,463 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 3 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='output', partition=4}, KafkaTopicPartition{topic='output', partition=0}]
2020-02-28 02:14:49,461 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 1 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='_input1', partition=7}, KafkaTopicPartition{topic='_input1', partition=3}]
2020-02-28 02:14:49,460 [Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 0 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='output', partition=1}, KafkaTopicPartition{topic='output', partition=5}]
2020-02-28 02:14:49,460 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 3 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='_input1', partition=5}, KafkaTopicPartition{topic='_input1', partition=1}]
2020-02-28 02:14:49,458 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 0 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='_input1', partition=6}, KafkaTopicPartition{topic='_input1', partition=2}]
2020-02-28 02:14:49,458 [Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 2 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='_input1', partition=4}, KafkaTopicPartition{topic='_input1', partition=0}]
2020-02-28 02:14:49,458 [Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 1 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='_input1', partition=7}, KafkaTopicPartition{topic='_input1', partition=3}]
2020-02-28 02:14:49,464 [Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 3 will start reading the following 2 partitions from the earliest offsets: [KafkaTopicPartition{topic='output', partition=4}, KafkaTopicPartition{topic='output', partition=0}]
2020-02-28 02:14:49,497 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 1 creating fetcher with offsets {KafkaTopicPartition{topic='_input1', partition=7}=-915623761775, KafkaTopicPartition{topic='_input1', partition=3}=-915623761775}.
2020-02-28 02:14:49,497 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='_input1', partition=6}=-915623761775, KafkaTopicPartition{topic='_input1', partition=2}=-915623761775}.
2020-02-28 02:14:49,497 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 2 creating fetcher with offsets {KafkaTopicPartition{topic='_input1', partition=4}=-915623761775, KafkaTopicPartition{topic='_input1', partition=0}=-915623761775}.
2020-02-28 02:14:49,497 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 3 creating fetcher with offsets {KafkaTopicPartition{topic='_input1', partition=5}=-915623761775, KafkaTopicPartition{topic='_input1', partition=1}=-915623761775}.
2020-02-28 02:14:49,497 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 2 creating fetcher with offsets {KafkaTopicPartition{topic='output', partition=3}=-915623761775, KafkaTopicPartition{topic='output', partition=7}=-915623761775}.
2020-02-28 02:14:49,497 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 3 creating fetcher with offsets {KafkaTopicPartition{topic='output', partition=4}=-915623761775, KafkaTopicPartition{topic='output', partition=0}=-915623761775}.
2020-02-28 02:14:49,498 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 1 creating fetcher with offsets {KafkaTopicPartition{topic='output', partition=2}=-915623761775, KafkaTopicPartition{topic='output', partition=6}=-915623761775}.
2020-02-28 02:14:49,498 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='output', partition=1}=-915623761775, KafkaTopicPartition{topic='output', partition=5}=-915623761775}.
2020-02-28 02:14:49,498 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 1 creating fetcher with offsets {KafkaTopicPartition{topic='_input1', partition=7}=-915623761775, KafkaTopicPartition{topic='_input1', partition=3}=-915623761775}.
2020-02-28 02:14:49,498 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 1 creating fetcher with offsets {KafkaTopicPartition{topic='output', partition=2}=-915623761775, KafkaTopicPartition{topic='output', partition=6}=-915623761775}.
2020-02-28 02:14:49,498 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 2 creating fetcher with offsets {KafkaTopicPartition{topic='output', partition=3}=-915623761775, KafkaTopicPartition{topic='output', partition=7}=-915623761775}.
2020-02-28 02:14:49,498 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 3 creating fetcher with offsets {KafkaTopicPartition{topic='_input1', partition=5}=-915623761775, KafkaTopicPartition{topic='_input1', partition=1}=-915623761775}.
2020-02-28 02:14:49,498 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 2 creating fetcher with offsets {KafkaTopicPartition{topic='_input1', partition=4}=-915623761775, KafkaTopicPartition{topic='_input1', partition=0}=-915623761775}.
2020-02-28 02:14:49,498 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 3 creating fetcher with offsets {KafkaTopicPartition{topic='output', partition=4}=-915623761775, KafkaTopicPartition{topic='output', partition=0}=-915623761775}.
2020-02-28 02:14:49,498 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='output', partition=1}=-915623761775, KafkaTopicPartition{topic='output', partition=5}=-915623761775}.
2020-02-28 02:14:49,500 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='_input1', partition=6}=-915623761775, KafkaTopicPartition{topic='_input1', partition=2}=-915623761775}.
2020-02-28 02:14:49,509 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,512 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,512 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,512 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889512
2020-02-28 02:14:49,517 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,529 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,533 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-17, groupId=KafkaCsvProducer] Subscribed to partition(s): _input1-6, _input1-2
2020-02-28 02:14:49,529 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,529 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,528 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,528 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,526 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,525 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,525 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,525 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,524 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,521 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,521 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,546 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,542 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,552 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,552 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889542
2020-02-28 02:14:49,540 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-17, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-6
2020-02-28 02:14:49,537 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-28 02:14:49,561 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-23, groupId=KafkaCsvProducer] Subscribed to partition(s): _input1-5, _input1-1
2020-02-28 02:14:49,562 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-23, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-5
2020-02-28 02:14:49,563 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,567 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,567 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889559
2020-02-28 02:14:49,575 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,578 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,578 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889557
2020-02-28 02:14:49,577 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-29, groupId=KafkaCsvProducer] Subscribed to partition(s): output-1, output-5
2020-02-28 02:14:49,578 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-29, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-1
2020-02-28 02:14:49,580 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-23, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,581 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,581 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,581 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889580
2020-02-28 02:14:49,581 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-23, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,583 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-19, groupId=KafkaCsvProducer] Subscribed to partition(s): _input1-5, _input1-1
2020-02-28 02:14:49,584 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,584 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-17, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,583 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-20, groupId=KafkaCsvProducer] Subscribed to partition(s): output-3, output-7
2020-02-28 02:14:49,608 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,609 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889556
2020-02-28 02:14:49,586 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-19, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-5
2020-02-28 02:14:49,609 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,609 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,609 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889554
2020-02-28 02:14:49,609 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,609 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-20, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-3
2020-02-28 02:14:49,611 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-31, groupId=KafkaCsvProducer] Subscribed to partition(s): output-3, output-7
2020-02-28 02:14:49,611 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-31, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-3
2020-02-28 02:14:49,612 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-29, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,608 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-17, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,610 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,613 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-29, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,612 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-27, groupId=KafkaCsvProducer] Subscribed to partition(s): output-4, output-0
2020-02-28 02:14:49,615 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-23, groupId=KafkaCsvProducer] Found no committed offset for partition _input1-1
2020-02-28 02:14:49,614 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889549
2020-02-28 02:14:49,616 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,616 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,616 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889547
2020-02-28 02:14:49,615 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-27, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-4
2020-02-28 02:14:49,621 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-19, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,622 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-19, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,618 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-29, groupId=KafkaCsvProducer] Found no committed offset for partition output-5
2020-02-28 02:14:49,623 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-27, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,622 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-20, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,624 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-27, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,624 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-24, groupId=KafkaCsvProducer] Subscribed to partition(s): _input1-7, _input1-3
2020-02-28 02:14:49,626 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-24, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-7
2020-02-28 02:14:49,624 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-20, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,627 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,627 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,628 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889626
2020-02-28 02:14:49,627 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-17, groupId=KafkaCsvProducer] Found no committed offset for partition _input1-2
2020-02-28 02:14:49,631 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-31, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,631 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-24, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,632 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-31, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,632 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-24, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,634 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,634 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,635 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889617
2020-02-28 02:14:49,636 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,636 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,636 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889609
2020-02-28 02:14:49,637 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-20, groupId=KafkaCsvProducer] Found no committed offset for partition output-7
2020-02-28 02:14:49,637 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,638 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,638 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889585
2020-02-28 02:14:49,639 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,639 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,639 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889581
2020-02-28 02:14:49,640 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,640 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,640 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889569
2020-02-28 02:14:49,640 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-28 02:14:49,641 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-28 02:14:49,641 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582848889634
2020-02-28 02:14:49,642 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-31, groupId=KafkaCsvProducer] Found no committed offset for partition output-7
2020-02-28 02:14:49,643 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-27, groupId=KafkaCsvProducer] Found no committed offset for partition output-0
2020-02-28 02:14:49,646 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-19, groupId=KafkaCsvProducer] Found no committed offset for partition _input1-1
2020-02-28 02:14:49,647 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-24, groupId=KafkaCsvProducer] Found no committed offset for partition _input1-3
2020-02-28 02:14:49,649 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-22, groupId=KafkaCsvProducer] Subscribed to partition(s): _input1-4, _input1-0
2020-02-28 02:14:49,650 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-22, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-4
2020-02-28 02:14:49,651 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-25, groupId=KafkaCsvProducer] Subscribed to partition(s): _input1-6, _input1-2
2020-02-28 02:14:49,651 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-25, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-6
2020-02-28 02:14:49,653 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-32, groupId=KafkaCsvProducer] Subscribed to partition(s): output-2, output-6
2020-02-28 02:14:49,653 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-32, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-2
2020-02-28 02:14:49,654 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-30, groupId=KafkaCsvProducer] Subscribed to partition(s): output-1, output-5
2020-02-28 02:14:49,654 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-30, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-1
2020-02-28 02:14:49,657 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-28, groupId=KafkaCsvProducer] Subscribed to partition(s): _input1-7, _input1-3
2020-02-28 02:14:49,657 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-28, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-7
2020-02-28 02:14:49,657 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-18, groupId=KafkaCsvProducer] Subscribed to partition(s): output-2, output-6
2020-02-28 02:14:49,659 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-18, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-2
2020-02-28 02:14:49,659 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-29, groupId=KafkaCsvProducer] Resetting offset for partition output-1 to offset 0.
2020-02-28 02:14:49,658 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-21, groupId=KafkaCsvProducer] Subscribed to partition(s): _input1-4, _input1-0
2020-02-28 02:14:49,660 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-21, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-4
2020-02-28 02:14:49,659 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-26, groupId=KafkaCsvProducer] Subscribed to partition(s): output-4, output-0
2020-02-28 02:14:49,661 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-26, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-4
2020-02-28 02:14:49,659 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-17, groupId=KafkaCsvProducer] Resetting offset for partition _input1-2 to offset 17.
2020-02-28 02:14:49,658 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-23, groupId=KafkaCsvProducer] Resetting offset for partition _input1-1 to offset 33.
2020-02-28 02:14:49,660 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-19, groupId=KafkaCsvProducer] Resetting offset for partition _input1-1 to offset 33.
2020-02-28 02:14:49,663 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-25, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,664 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-25, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,671 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-26, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,672 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-18, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,674 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-26, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,673 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-28, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,675 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-18, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,675 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-28, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,677 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-31, groupId=KafkaCsvProducer] Resetting offset for partition output-7 to offset 0.
2020-02-28 02:14:49,678 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-20, groupId=KafkaCsvProducer] Resetting offset for partition output-7 to offset 0.
2020-02-28 02:14:49,683 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-20, groupId=KafkaCsvProducer] Resetting offset for partition output-3 to offset 0.
2020-02-28 02:14:49,684 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-20, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-7
2020-02-28 02:14:49,685 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-25, groupId=KafkaCsvProducer] Found no committed offset for partition _input1-2
2020-02-28 02:14:49,690 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-22, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,691 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-22, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,692 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-21, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,693 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-21, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,697 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-26, groupId=KafkaCsvProducer] Found no committed offset for partition output-0
2020-02-28 02:14:49,698 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-22, groupId=KafkaCsvProducer] Found no committed offset for partition _input1-0
2020-02-28 02:14:49,699 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-21, groupId=KafkaCsvProducer] Found no committed offset for partition _input1-0
2020-02-28 02:14:49,698 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-18, groupId=KafkaCsvProducer] Found no committed offset for partition output-6
2020-02-28 02:14:49,700 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-28, groupId=KafkaCsvProducer] Found no committed offset for partition _input1-3
2020-02-28 02:14:49,703 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-24, groupId=KafkaCsvProducer] Resetting offset for partition _input1-3 to offset 12.
2020-02-28 02:14:49,703 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-24, groupId=KafkaCsvProducer] Resetting offset for partition _input1-7 to offset 0.
2020-02-28 02:14:49,703 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-24, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-3
2020-02-28 02:14:49,704 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-19, groupId=KafkaCsvProducer] Resetting offset for partition _input1-5 to offset 0.
2020-02-28 02:14:49,710 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-19, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-1
2020-02-28 02:14:49,712 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-19, groupId=KafkaCsvProducer] Resetting offset for partition _input1-1 to offset 0.
2020-02-28 02:14:49,704 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-25, groupId=KafkaCsvProducer] Resetting offset for partition _input1-2 to offset 17.
2020-02-28 02:14:49,715 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-25, groupId=KafkaCsvProducer] Resetting offset for partition _input1-6 to offset 0.
2020-02-28 02:14:49,716 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-25, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-2
2020-02-28 02:14:49,718 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-24, groupId=KafkaCsvProducer] Resetting offset for partition _input1-3 to offset 0.
2020-02-28 02:14:49,707 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-27, groupId=KafkaCsvProducer] Resetting offset for partition output-0 to offset 0.
2020-02-28 02:14:49,720 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-27, groupId=KafkaCsvProducer] Resetting offset for partition output-4 to offset 0.
2020-02-28 02:14:49,720 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-27, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-0
2020-02-28 02:14:49,706 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-31, groupId=KafkaCsvProducer] Resetting offset for partition output-3 to offset 0.
2020-02-28 02:14:49,722 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-31, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-7
2020-02-28 02:14:49,705 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-32, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,724 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-21, groupId=KafkaCsvProducer] Resetting offset for partition _input1-0 to offset 25.
2020-02-28 02:14:49,704 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-23, groupId=KafkaCsvProducer] Resetting offset for partition _input1-5 to offset 0.
2020-02-28 02:14:49,724 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-23, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-1
2020-02-28 02:14:49,705 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-17, groupId=KafkaCsvProducer] Resetting offset for partition _input1-6 to offset 0.
2020-02-28 02:14:49,728 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-17, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-2
2020-02-28 02:14:49,728 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-27, groupId=KafkaCsvProducer] Resetting offset for partition output-0 to offset 0.
2020-02-28 02:14:49,705 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-29, groupId=KafkaCsvProducer] Resetting offset for partition output-5 to offset 0.
2020-02-28 02:14:49,729 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-29, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-5
2020-02-28 02:14:49,728 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-26, groupId=KafkaCsvProducer] Resetting offset for partition output-0 to offset 0.
2020-02-28 02:14:49,724 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-32, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,732 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-29, groupId=KafkaCsvProducer] Resetting offset for partition output-5 to offset 0.
2020-02-28 02:14:49,734 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-31, groupId=KafkaCsvProducer] Resetting offset for partition output-7 to offset 0.
2020-02-28 02:14:49,735 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-25, groupId=KafkaCsvProducer] Resetting offset for partition _input1-2 to offset 0.
2020-02-28 02:14:49,724 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-21, groupId=KafkaCsvProducer] Resetting offset for partition _input1-4 to offset 0.
2020-02-28 02:14:49,718 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-28, groupId=KafkaCsvProducer] Resetting offset for partition _input1-3 to offset 12.
2020-02-28 02:14:49,736 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-28, groupId=KafkaCsvProducer] Resetting offset for partition _input1-7 to offset 0.
2020-02-28 02:14:49,736 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-28, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-3
2020-02-28 02:14:49,718 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-22, groupId=KafkaCsvProducer] Resetting offset for partition _input1-0 to offset 25.
2020-02-28 02:14:49,716 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-18, groupId=KafkaCsvProducer] Resetting offset for partition output-2 to offset 0.
2020-02-28 02:14:49,736 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-18, groupId=KafkaCsvProducer] Resetting offset for partition output-6 to offset 4.
2020-02-28 02:14:49,737 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-18, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-6
2020-02-28 02:14:49,716 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-20, groupId=KafkaCsvProducer] Resetting offset for partition output-7 to offset 0.
2020-02-28 02:14:49,710 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-30, groupId=KafkaCsvProducer] Cluster ID: AZDT7CeLRi2BFIxGgN344Q
2020-02-28 02:14:49,739 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-30, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-28 02:14:49,737 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-21, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-0
2020-02-28 02:14:49,736 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-22, groupId=KafkaCsvProducer] Resetting offset for partition _input1-4 to offset 0.
2020-02-28 02:14:49,736 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-23, groupId=KafkaCsvProducer] Resetting offset for partition _input1-1 to offset 0.
2020-02-28 02:14:49,733 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-17, groupId=KafkaCsvProducer] Resetting offset for partition _input1-2 to offset 0.
2020-02-28 02:14:49,755 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-18, groupId=KafkaCsvProducer] Resetting offset for partition output-6 to offset 0.
2020-02-28 02:14:49,733 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-32, groupId=KafkaCsvProducer] Found no committed offset for partition output-6
2020-02-28 02:14:49,729 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-26, groupId=KafkaCsvProducer] Resetting offset for partition output-4 to offset 0.
2020-02-28 02:14:49,757 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-26, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-0
2020-02-28 02:14:49,753 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (ConsumerCoordinator.java:1241) - [Consumer clientId=consumer-KafkaCsvProducer-30, groupId=KafkaCsvProducer] Found no committed offset for partition output-5
2020-02-28 02:14:49,749 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-22, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition _input1-0
2020-02-28 02:14:49,761 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-26, groupId=KafkaCsvProducer] Resetting offset for partition output-0 to offset 0.
2020-02-28 02:14:49,764 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-21, groupId=KafkaCsvProducer] Resetting offset for partition _input1-0 to offset 0.
2020-02-28 02:14:49,766 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-32, groupId=KafkaCsvProducer] Resetting offset for partition output-2 to offset 0.
2020-02-28 02:14:49,766 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-32, groupId=KafkaCsvProducer] Resetting offset for partition output-6 to offset 4.
2020-02-28 02:14:49,766 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-32, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-6
2020-02-28 02:14:49,765 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-30, groupId=KafkaCsvProducer] Resetting offset for partition output-1 to offset 0.
2020-02-28 02:14:49,766 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-30, groupId=KafkaCsvProducer] Resetting offset for partition output-5 to offset 0.
2020-02-28 02:14:49,766 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-30, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition output-5
2020-02-28 02:14:49,765 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-22, groupId=KafkaCsvProducer] Resetting offset for partition _input1-0 to offset 0.
2020-02-28 02:14:49,770 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-30, groupId=KafkaCsvProducer] Resetting offset for partition output-5 to offset 0.
2020-02-28 02:14:49,771 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Map (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-28, groupId=KafkaCsvProducer] Resetting offset for partition _input1-3 to offset 0.
2020-02-28 02:14:49,775 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-32, groupId=KafkaCsvProducer] Resetting offset for partition output-6 to offset 0.
