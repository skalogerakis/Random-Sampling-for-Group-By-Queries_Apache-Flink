2020-02-26 19:55:00,000 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Sink: Print to Std. Out (4/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,001 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (bbfe283558832dcac4f09e28d85c5526) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,001 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,002 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (20ea73ab2ad5a91794bbd0dce8d5b9ff) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,002 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,003 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (d2eae12a45cbcd2a1d07c3911c11f155) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,003 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,003 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (393e3715c0a4d41dd93f9f59f5c0da53) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,004 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,004 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (4903cd4965f47513b4382a39d483399a) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,004 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,006 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (d420469b500877dee69c9ffd7f0f8ac1) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,006 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,006 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (ea317ba850bb8ba3d000d16644cf15f5) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,006 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,007 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (43509d45a2b672c7d4024174b67188a3) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,007 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,007 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (164238a0cd9f934f1a605697aac4a5da) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,008 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,008 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (5298c490721b52ef3519356a6550c579) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,009 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,009 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (a7496c1e051450fbd9d6098cad86a447) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,009 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,010 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (477182de26a504ff85cdc4c68368b462) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,010 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,010 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (1f09606138e56cc2f2e958d39aed340f) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,010 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,011 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (379991634adf113a7cc23f831284f3b8) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,011 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,012 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (fca4cc6e57cec7861a00295e1ce1fdc5) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,012 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,012 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (369d71c8b43a651e1150e94cb0e3128c) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,014 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,014 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (1/4) (14a97f40f411a249dbcd17f3b16911e4) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,015 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Flat Map (1/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,015 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (2/4) (8bcb7c2b3ed62fd217790959b620e9f8) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,016 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Flat Map (2/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,016 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (3/4) (44dfa1a82d42f3faf12cf8c9e8fd1f5c) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,016 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Flat Map (3/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,016 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Flat Map (4/4) (e40cc32a76f5a2b0dc798efd84374265) switched from SCHEDULED to DEPLOYING.
2020-02-26 19:55:00,017 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:713) - Deploying Flat Map (4/4) (attempt #0) to 95ce5736-8224-4ec4-bede-481a9a6f99c2 @ localhost (dataPort=-1)
2020-02-26 19:55:00,027 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (1/4).
2020-02-26 19:55:00,027 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (6f16404a78e2fff4877ec1d8ddf47ded) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,028 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (1/4) (6f16404a78e2fff4877ec1d8ddf47ded) [DEPLOYING]
2020-02-26 19:55:00,034 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (2/4).
2020-02-26 19:55:00,034 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (1/4) (6f16404a78e2fff4877ec1d8ddf47ded) [DEPLOYING].
2020-02-26 19:55:00,034 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (f82fddfa25e3499a5821a49a7ee6e45e) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,035 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (2/4) (f82fddfa25e3499a5821a49a7ee6e45e) [DEPLOYING]
2020-02-26 19:55:00,037 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (2/4) (f82fddfa25e3499a5821a49a7ee6e45e) [DEPLOYING].
2020-02-26 19:55:00,039 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (2/4) (f82fddfa25e3499a5821a49a7ee6e45e) [DEPLOYING].
2020-02-26 19:55:00,042 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (3/4).
2020-02-26 19:55:00,037 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (1/4) (6f16404a78e2fff4877ec1d8ddf47ded) [DEPLOYING].
2020-02-26 19:55:00,045 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Sink: Print to Std. Out (4/4).
2020-02-26 19:55:00,046 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (44e2d87dc2475f89580f778d8e9aa325) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,046 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (3/4) (44e2d87dc2475f89580f778d8e9aa325) [DEPLOYING]
2020-02-26 19:55:00,047 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (3/4) (44e2d87dc2475f89580f778d8e9aa325) [DEPLOYING].
2020-02-26 19:55:00,047 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (27ddaaee37167b5cf34922822409eaf1) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,051 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Sink: Print to Std. Out (4/4) (27ddaaee37167b5cf34922822409eaf1) [DEPLOYING]
2020-02-26 19:55:00,051 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Sink: Print to Std. Out (4/4) (27ddaaee37167b5cf34922822409eaf1) [DEPLOYING].
2020-02-26 19:55:00,052 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (3/4) (44e2d87dc2475f89580f778d8e9aa325) [DEPLOYING].
2020-02-26 19:55:00,052 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (f82fddfa25e3499a5821a49a7ee6e45e) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,052 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Sink: Print to Std. Out (4/4) (27ddaaee37167b5cf34922822409eaf1) [DEPLOYING].
2020-02-26 19:55:00,053 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (2/4) (f82fddfa25e3499a5821a49a7ee6e45e) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,053 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4).
2020-02-26 19:55:00,052 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (6f16404a78e2fff4877ec1d8ddf47ded) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,054 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (1/4) (6f16404a78e2fff4877ec1d8ddf47ded) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,054 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,054 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (27ddaaee37167b5cf34922822409eaf1) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,054 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,055 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,055 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (4/4) (27ddaaee37167b5cf34922822409eaf1) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,056 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (20ea73ab2ad5a91794bbd0dce8d5b9ff) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,061 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4).
2020-02-26 19:55:00,059 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (44e2d87dc2475f89580f778d8e9aa325) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,064 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,064 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Source: Custom Source -> Sink: Print to Std. Out (3/4) (44e2d87dc2475f89580f778d8e9aa325) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,066 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (20ea73ab2ad5a91794bbd0dce8d5b9ff) [DEPLOYING]
2020-02-26 19:55:00,066 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (20ea73ab2ad5a91794bbd0dce8d5b9ff) [DEPLOYING].
2020-02-26 19:55:00,067 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (20ea73ab2ad5a91794bbd0dce8d5b9ff) [DEPLOYING].
2020-02-26 19:55:00,076 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (bbfe283558832dcac4f09e28d85c5526) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,076 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (bbfe283558832dcac4f09e28d85c5526) [DEPLOYING]
2020-02-26 19:55:00,076 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (bbfe283558832dcac4f09e28d85c5526) [DEPLOYING].
2020-02-26 19:55:00,077 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (bbfe283558832dcac4f09e28d85c5526) [DEPLOYING].
2020-02-26 19:55:00,086 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (20ea73ab2ad5a91794bbd0dce8d5b9ff) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,087 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,088 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4).
2020-02-26 19:55:00,087 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4) (20ea73ab2ad5a91794bbd0dce8d5b9ff) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,091 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (d2eae12a45cbcd2a1d07c3911c11f155) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,091 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (d2eae12a45cbcd2a1d07c3911c11f155) [DEPLOYING]
2020-02-26 19:55:00,092 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (bbfe283558832dcac4f09e28d85c5526) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,093 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4) (bbfe283558832dcac4f09e28d85c5526) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,093 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,098 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (d2eae12a45cbcd2a1d07c3911c11f155) [DEPLOYING].
2020-02-26 19:55:00,100 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (d2eae12a45cbcd2a1d07c3911c11f155) [DEPLOYING].
2020-02-26 19:55:00,103 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4).
2020-02-26 19:55:00,110 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-26 19:55:00,116 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 1/4 - no state to restore
2020-02-26 19:55:00,117 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-26 19:55:00,117 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-26 19:55:00,117 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-26 19:55:00,119 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-26 19:55:00,120 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 0/4 - no state to restore
2020-02-26 19:55:00,121 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (d2eae12a45cbcd2a1d07c3911c11f155) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,121 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4) (d2eae12a45cbcd2a1d07c3911c11f155) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,122 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (393e3715c0a4d41dd93f9f59f5c0da53) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,122 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (393e3715c0a4d41dd93f9f59f5c0da53) [DEPLOYING]
2020-02-26 19:55:00,122 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Task.java:593) - Loading JAR files for task Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (393e3715c0a4d41dd93f9f59f5c0da53) [DEPLOYING].
2020-02-26 19:55:00,123 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Task.java:619) - Registering task at network: Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (393e3715c0a4d41dd93f9f59f5c0da53) [DEPLOYING].
2020-02-26 19:55:00,117 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-26 19:55:00,123 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-26 19:55:00,122 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4).
2020-02-26 19:55:00,122 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,117 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-26 19:55:00,125 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-26 19:55:00,125 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-26 19:55:00,133 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 3 has no restore state.
2020-02-26 19:55:00,133 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 1 has no restore state.
2020-02-26 19:55:00,133 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (4903cd4965f47513b4382a39d483399a) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,133 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (4903cd4965f47513b4382a39d483399a) [DEPLOYING]
2020-02-26 19:55:00,139 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (4903cd4965f47513b4382a39d483399a) [DEPLOYING].
2020-02-26 19:55:00,136 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-26 19:55:00,139 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 2/4 - no state to restore
2020-02-26 19:55:00,136 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4).
2020-02-26 19:55:00,136 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Task.java:958) - Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (393e3715c0a4d41dd93f9f59f5c0da53) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,140 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (4903cd4965f47513b4382a39d483399a) [DEPLOYING].
2020-02-26 19:55:00,140 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4) (393e3715c0a4d41dd93f9f59f5c0da53) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,140 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,134 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 0 has no restore state.
2020-02-26 19:55:00,145 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-26 19:55:00,134 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 2 has no restore state.
2020-02-26 19:55:00,145 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 3/4 - no state to restore
2020-02-26 19:55:00,163 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (d420469b500877dee69c9ffd7f0f8ac1) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,163 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (d420469b500877dee69c9ffd7f0f8ac1) [DEPLOYING]
2020-02-26 19:55:00,164 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (d420469b500877dee69c9ffd7f0f8ac1) [DEPLOYING].
2020-02-26 19:55:00,164 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (d420469b500877dee69c9ffd7f0f8ac1) [DEPLOYING].
2020-02-26 19:55:00,165 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4).
2020-02-26 19:55:00,166 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (4903cd4965f47513b4382a39d483399a) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,167 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,167 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4) (4903cd4965f47513b4382a39d483399a) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,170 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4).
2020-02-26 19:55:00,171 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (ea317ba850bb8ba3d000d16644cf15f5) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,171 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (ea317ba850bb8ba3d000d16644cf15f5) [DEPLOYING]
2020-02-26 19:55:00,171 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (ea317ba850bb8ba3d000d16644cf15f5) [DEPLOYING].
2020-02-26 19:55:00,177 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-26 19:55:00,177 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,177 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,178 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,178 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-26 19:55:00,180 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,183 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (43509d45a2b672c7d4024174b67188a3) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,184 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-26 19:55:00,184 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-26 19:55:00,184 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (43509d45a2b672c7d4024174b67188a3) [DEPLOYING]
2020-02-26 19:55:00,185 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (43509d45a2b672c7d4024174b67188a3) [DEPLOYING].
2020-02-26 19:55:00,186 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (43509d45a2b672c7d4024174b67188a3) [DEPLOYING].
2020-02-26 19:55:00,186 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (ea317ba850bb8ba3d000d16644cf15f5) [DEPLOYING].
2020-02-26 19:55:00,198 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4).
2020-02-26 19:55:00,214 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4).
2020-02-26 19:55:00,214 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (d420469b500877dee69c9ffd7f0f8ac1) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,215 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4) (d420469b500877dee69c9ffd7f0f8ac1) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,215 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,219 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (5298c490721b52ef3519356a6550c579) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,219 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (5298c490721b52ef3519356a6550c579) [DEPLOYING]
2020-02-26 19:55:00,219 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (5298c490721b52ef3519356a6550c579) [DEPLOYING].
2020-02-26 19:55:00,225 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (5298c490721b52ef3519356a6550c579) [DEPLOYING].
2020-02-26 19:55:00,230 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (43509d45a2b672c7d4024174b67188a3) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,230 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (ea317ba850bb8ba3d000d16644cf15f5) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,230 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4) (43509d45a2b672c7d4024174b67188a3) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,230 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,231 [flink-akka.actor.default-dispatcher-5] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4) (ea317ba850bb8ba3d000d16644cf15f5) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,230 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,248 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (5298c490721b52ef3519356a6550c579) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,248 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4) (5298c490721b52ef3519356a6550c579) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,249 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,249 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4).
2020-02-26 19:55:00,251 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4).
2020-02-26 19:55:00,254 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (a7496c1e051450fbd9d6098cad86a447) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,257 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (a7496c1e051450fbd9d6098cad86a447) [DEPLOYING]
2020-02-26 19:55:00,257 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (a7496c1e051450fbd9d6098cad86a447) [DEPLOYING].
2020-02-26 19:55:00,258 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (a7496c1e051450fbd9d6098cad86a447) [DEPLOYING].
2020-02-26 19:55:00,257 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (1f09606138e56cc2f2e958d39aed340f) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,256 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4).
2020-02-26 19:55:00,259 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (1f09606138e56cc2f2e958d39aed340f) [DEPLOYING]
2020-02-26 19:55:00,259 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (1f09606138e56cc2f2e958d39aed340f) [DEPLOYING].
2020-02-26 19:55:00,260 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (1f09606138e56cc2f2e958d39aed340f) [DEPLOYING].
2020-02-26 19:55:00,266 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] WARN  (TaskMetricGroup.java:143) - The operator name Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) exceeded the 80 characters length limit and was truncated.
2020-02-26 19:55:00,254 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (477182de26a504ff85cdc4c68368b462) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,268 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (477182de26a504ff85cdc4c68368b462) [DEPLOYING]
2020-02-26 19:55:00,268 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (477182de26a504ff85cdc4c68368b462) [DEPLOYING].
2020-02-26 19:55:00,268 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (477182de26a504ff85cdc4c68368b462) [DEPLOYING].
2020-02-26 19:55:00,275 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (a7496c1e051450fbd9d6098cad86a447) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,275 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4) (a7496c1e051450fbd9d6098cad86a447) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,276 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,278 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] WARN  (TaskMetricGroup.java:143) - The operator name Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) exceeded the 80 characters length limit and was truncated.
2020-02-26 19:55:00,281 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,282 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (1f09606138e56cc2f2e958d39aed340f) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,281 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,282 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (477182de26a504ff85cdc4c68368b462) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,283 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,284 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4) (1f09606138e56cc2f2e958d39aed340f) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,285 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4) (477182de26a504ff85cdc4c68368b462) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,281 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,281 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,281 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,281 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, initAggrWindow) -> (Flat Map, Map) (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,285 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] WARN  (TaskMetricGroup.java:143) - The operator name Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) exceeded the 80 characters length limit and was truncated.
2020-02-26 19:55:00,283 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,291 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,309 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,309 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,309 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700307
2020-02-26 19:55:00,315 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (3/4) to produce into default topic _input-topic-job1
2020-02-26 19:55:00,316 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-26 19:55:00,316 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 0/4 - no state to restore
2020-02-26 19:55:00,317 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-26 19:55:00,319 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-26 19:55:00,319 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-26 19:55:00,321 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 2 has no restore state.
2020-02-26 19:55:00,321 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,322 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,322 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700321
2020-02-26 19:55:00,322 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (379991634adf113a7cc23f831284f3b8) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,323 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (379991634adf113a7cc23f831284f3b8) [DEPLOYING]
2020-02-26 19:55:00,323 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (379991634adf113a7cc23f831284f3b8) [DEPLOYING].
2020-02-26 19:55:00,323 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (379991634adf113a7cc23f831284f3b8) [DEPLOYING].
2020-02-26 19:55:00,327 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,328 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4).
2020-02-26 19:55:00,329 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (379991634adf113a7cc23f831284f3b8) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,330 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,330 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4) (379991634adf113a7cc23f831284f3b8) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,332 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (4/4) to produce into default topic _input-topic-job1
2020-02-26 19:55:00,332 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,332 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,332 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700328
2020-02-26 19:55:00,333 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-26 19:55:00,333 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-26 19:55:00,333 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,333 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,333 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700328
2020-02-26 19:55:00,334 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (2/4) to produce into default topic _input-topic-job1
2020-02-26 19:55:00,335 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-26 19:55:00,336 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-26 19:55:00,337 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 1 has no restore state.
2020-02-26 19:55:00,338 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,333 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (1/4) to produce into default topic _input-topic-job1
2020-02-26 19:55:00,333 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 3 has no restore state.
2020-02-26 19:55:00,339 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,339 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (TypeExtractor.java:1818) - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
2020-02-26 19:55:00,340 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (TypeExtractor.java:1857) - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
2020-02-26 19:55:00,340 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (FlinkKafkaConsumerBase.java:886) - Consumer subtask 0 has no restore state.
2020-02-26 19:55:00,341 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,346 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4).
2020-02-26 19:55:00,346 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (164238a0cd9f934f1a605697aac4a5da) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,347 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-26 19:55:00,348 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 1/4 - no state to restore
2020-02-26 19:55:00,348 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (164238a0cd9f934f1a605697aac4a5da) [DEPLOYING]
2020-02-26 19:55:00,348 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (164238a0cd9f934f1a605697aac4a5da) [DEPLOYING].
2020-02-26 19:55:00,349 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-26 19:55:00,349 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (164238a0cd9f934f1a605697aac4a5da) [DEPLOYING].
2020-02-26 19:55:00,354 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4).
2020-02-26 19:55:00,354 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (fca4cc6e57cec7861a00295e1ce1fdc5) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,357 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (fca4cc6e57cec7861a00295e1ce1fdc5) [DEPLOYING]
2020-02-26 19:55:00,357 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (fca4cc6e57cec7861a00295e1ce1fdc5) [DEPLOYING].
2020-02-26 19:55:00,358 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (fca4cc6e57cec7861a00295e1ce1fdc5) [DEPLOYING].
2020-02-26 19:55:00,361 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (369d71c8b43a651e1150e94cb0e3128c) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,361 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (369d71c8b43a651e1150e94cb0e3128c) [DEPLOYING]
2020-02-26 19:55:00,362 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (Task.java:593) - Loading JAR files for task Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (369d71c8b43a651e1150e94cb0e3128c) [DEPLOYING].
2020-02-26 19:55:00,362 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (fca4cc6e57cec7861a00295e1ce1fdc5) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,363 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4) (fca4cc6e57cec7861a00295e1ce1fdc5) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,362 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (Task.java:619) - Registering task at network: Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (369d71c8b43a651e1150e94cb0e3128c) [DEPLOYING].
2020-02-26 19:55:00,363 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,364 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (369d71c8b43a651e1150e94cb0e3128c) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,364 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4) (369d71c8b43a651e1150e94cb0e3128c) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,366 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (Task.java:958) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (164238a0cd9f934f1a605697aac4a5da) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,365 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,367 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,367 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700365
2020-02-26 19:55:00,364 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,367 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,371 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] WARN  (TaskMetricGroup.java:143) - The operator name Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) exceeded the 80 characters length limit and was truncated.
2020-02-26 19:55:00,372 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,367 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Window(TumblingEventTimeWindows(30000), EventTimeTrigger, SumAggregator, PassThroughWindowFunction) -> Map (1/4) (164238a0cd9f934f1a605697aac4a5da) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,372 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-26 19:55:00,373 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 3/4 - no state to restore
2020-02-26 19:55:00,373 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-26 19:55:00,374 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Flat Map (1/4).
2020-02-26 19:55:00,374 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,374 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,374 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700365
2020-02-26 19:55:00,381 [Flat Map (1/4)] INFO  (Task.java:958) - Flat Map (1/4) (14a97f40f411a249dbcd17f3b16911e4) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,390 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Flat Map (2/4).
2020-02-26 19:55:00,386 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,393 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,393 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700375
2020-02-26 19:55:00,393 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] WARN  (FlinkKafkaProducer.java:998) - Using EXACTLY_ONCE semantic, but checkpointing is not enabled. Switching to NONE semantic.
2020-02-26 19:55:00,393 [Flat Map (1/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (1/4) (14a97f40f411a249dbcd17f3b16911e4) [DEPLOYING]
2020-02-26 19:55:00,393 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (TwoPhaseCommitSinkFunction.java:366) - FlinkKafkaProducer 2/4 - no state to restore
2020-02-26 19:55:00,393 [Flat Map (1/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (1/4) (14a97f40f411a249dbcd17f3b16911e4) [DEPLOYING].
2020-02-26 19:55:00,394 [Flat Map (1/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (1/4) (14a97f40f411a249dbcd17f3b16911e4) [DEPLOYING].
2020-02-26 19:55:00,394 [Flat Map (1/4)] INFO  (Task.java:958) - Flat Map (1/4) (14a97f40f411a249dbcd17f3b16911e4) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,395 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Flat Map (1/4) (14a97f40f411a249dbcd17f3b16911e4) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,394 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (AbstractConfig.java:347) - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-02-26 19:55:00,396 [Flat Map (1/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,401 [Flat Map (2/4)] INFO  (Task.java:958) - Flat Map (2/4) (8bcb7c2b3ed62fd217790959b620e9f8) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,403 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,405 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,405 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700401
2020-02-26 19:55:00,402 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Flat Map (3/4).
2020-02-26 19:55:00,406 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,406 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,406 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700395
2020-02-26 19:55:00,406 [Flat Map (2/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (2/4) (8bcb7c2b3ed62fd217790959b620e9f8) [DEPLOYING]
2020-02-26 19:55:00,406 [Flat Map (2/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (2/4) (8bcb7c2b3ed62fd217790959b620e9f8) [DEPLOYING].
2020-02-26 19:55:00,408 [Flat Map (2/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (2/4) (8bcb7c2b3ed62fd217790959b620e9f8) [DEPLOYING].
2020-02-26 19:55:00,407 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,408 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,408 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700392
2020-02-26 19:55:00,414 [Flat Map (2/4)] INFO  (Task.java:958) - Flat Map (2/4) (8bcb7c2b3ed62fd217790959b620e9f8) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,415 [flink-akka.actor.default-dispatcher-2] INFO  (Execution.java:1509) - Flat Map (2/4) (8bcb7c2b3ed62fd217790959b620e9f8) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,415 [flink-akka.actor.default-dispatcher-4] INFO  (TaskExecutor.java:592) - Received task Flat Map (4/4).
2020-02-26 19:55:00,415 [Flat Map (2/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,415 [Flat Map (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,416 [flink-akka.actor.default-dispatcher-4] INFO  (TaskSlotTable.java:242) - Activate slot ffcc811d3e0ae679b86b55512318ca80.
2020-02-26 19:55:00,416 [Flat Map (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,416 [flink-akka.actor.default-dispatcher-4] INFO  (TaskSlotTable.java:242) - Activate slot ce9f902a1d28d87108d7cefd77277e21.
2020-02-26 19:55:00,417 [flink-akka.actor.default-dispatcher-4] INFO  (TaskSlotTable.java:242) - Activate slot 68f3d785851d47ac5a8a8fb8d4c56c99.
2020-02-26 19:55:00,417 [flink-akka.actor.default-dispatcher-4] INFO  (TaskSlotTable.java:242) - Activate slot 2c41dba988fb9051d963cd8269abcf05.
2020-02-26 19:55:00,417 [Flat Map (4/4)] INFO  (Task.java:958) - Flat Map (4/4) (e40cc32a76f5a2b0dc798efd84374265) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,417 [Flat Map (4/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (4/4) (e40cc32a76f5a2b0dc798efd84374265) [DEPLOYING]
2020-02-26 19:55:00,417 [Flat Map (4/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (4/4) (e40cc32a76f5a2b0dc798efd84374265) [DEPLOYING].
2020-02-26 19:55:00,418 [Flat Map (4/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (4/4) (e40cc32a76f5a2b0dc798efd84374265) [DEPLOYING].
2020-02-26 19:55:00,423 [Flat Map (3/4)] INFO  (Task.java:958) - Flat Map (3/4) (44dfa1a82d42f3faf12cf8c9e8fd1f5c) switched from CREATED to DEPLOYING.
2020-02-26 19:55:00,424 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,424 [Flat Map (3/4)] INFO  (Task.java:586) - Creating FileSystem stream leak safety net for task Flat Map (3/4) (44dfa1a82d42f3faf12cf8c9e8fd1f5c) [DEPLOYING]
2020-02-26 19:55:00,424 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,424 [Flat Map (3/4)] INFO  (Task.java:593) - Loading JAR files for task Flat Map (3/4) (44dfa1a82d42f3faf12cf8c9e8fd1f5c) [DEPLOYING].
2020-02-26 19:55:00,425 [Flat Map (3/4)] INFO  (Task.java:619) - Registering task at network: Flat Map (3/4) (44dfa1a82d42f3faf12cf8c9e8fd1f5c) [DEPLOYING].
2020-02-26 19:55:00,424 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700386
2020-02-26 19:55:00,428 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (2/4) to produce into default topic output-topic-job1
2020-02-26 19:55:00,429 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,429 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,429 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700426
2020-02-26 19:55:00,429 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (2/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,430 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,430 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,430 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700418
2020-02-26 19:55:00,430 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,431 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,431 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700404
2020-02-26 19:55:00,431 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (3/4) to produce into default topic output-topic-job1
2020-02-26 19:55:00,432 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,435 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,436 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,436 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700435
2020-02-26 19:55:00,436 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (4/4) to produce into default topic output-topic-job1
2020-02-26 19:55:00,444 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,444 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,444 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700443
2020-02-26 19:55:00,444 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,440 [Flat Map (4/4)] INFO  (Task.java:958) - Flat Map (4/4) (e40cc32a76f5a2b0dc798efd84374265) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,445 [Flat Map (4/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,444 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (FlinkKafkaProducer.java:1158) - Starting FlinkKafkaInternalProducer (1/4) to produce into default topic output-topic-job1
2020-02-26 19:55:00,447 [Window(TumblingEventTimeWindows(30000), EventTimeTrigger, CoGroupWindowFunction) -> Sink: Unnamed (1/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,448 [flink-akka.actor.default-dispatcher-3] INFO  (Execution.java:1509) - Flat Map (4/4) (e40cc32a76f5a2b0dc798efd84374265) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,446 [Flat Map (3/4)] INFO  (Task.java:958) - Flat Map (3/4) (44dfa1a82d42f3faf12cf8c9e8fd1f5c) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,449 [Flat Map (3/4)] INFO  (StateBackendLoader.java:227) - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
2020-02-26 19:55:00,450 [flink-akka.actor.default-dispatcher-4] INFO  (Execution.java:1509) - Flat Map (3/4) (44dfa1a82d42f3faf12cf8c9e8fd1f5c) switched from DEPLOYING to RUNNING.
2020-02-26 19:55:00,452 [Flat Map (3/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,452 [Flat Map (4/4)] INFO  (HeapKeyedStateBackend.java:140) - Initializing heap keyed state backend with stream factory.
2020-02-26 19:55:00,780 [kafka-producer-network-thread | producer-4] INFO  (Metadata.java:261) - [Producer clientId=producer-4] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [kafka-producer-network-thread | producer-6] INFO  (Metadata.java:261) - [Producer clientId=producer-6] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [kafka-producer-network-thread | producer-3] INFO  (Metadata.java:261) - [Producer clientId=producer-3] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-4, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [kafka-producer-network-thread | producer-8] INFO  (Metadata.java:261) - [Producer clientId=producer-8] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [kafka-producer-network-thread | producer-1] INFO  (Metadata.java:261) - [Producer clientId=producer-1] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-1, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,780 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-2, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,780 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-7, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-5, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,780 [kafka-producer-network-thread | producer-7] INFO  (Metadata.java:261) - [Producer clientId=producer-7] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [kafka-producer-network-thread | producer-2] INFO  (Metadata.java:261) - [Producer clientId=producer-2] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [kafka-producer-network-thread | producer-5] INFO  (Metadata.java:261) - [Producer clientId=producer-5] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-3, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-8, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,781 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-6, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,786 [Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 0 will start reading the following 1 partitions from the earliest offsets: [KafkaTopicPartition{topic='input-topic-job1', partition=1}]
2020-02-26 19:55:00,786 [Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 0 will start reading the following 1 partitions from the earliest offsets: [KafkaTopicPartition{topic='input-topic-job1', partition=1}]
2020-02-26 19:55:00,786 [Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 3 will start reading the following 1 partitions from the earliest offsets: [KafkaTopicPartition{topic='input-topic-job1', partition=0}]
2020-02-26 19:55:00,786 [Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 3 will start reading the following 1 partitions from the earliest offsets: [KafkaTopicPartition{topic='input-topic-job1', partition=0}]
2020-02-26 19:55:00,786 [Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 1 will start reading the following 1 partitions from the earliest offsets: [KafkaTopicPartition{topic='input-topic-job1', partition=2}]
2020-02-26 19:55:00,786 [Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 2 will start reading the following 1 partitions from the earliest offsets: [KafkaTopicPartition{topic='input-topic-job1', partition=3}]
2020-02-26 19:55:00,786 [Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 1 will start reading the following 1 partitions from the earliest offsets: [KafkaTopicPartition{topic='input-topic-job1', partition=2}]
2020-02-26 19:55:00,786 [Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (FlinkKafkaConsumerBase.java:604) - Consumer subtask 2 will start reading the following 1 partitions from the earliest offsets: [KafkaTopicPartition{topic='input-topic-job1', partition=3}]
2020-02-26 19:55:00,793 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='input-topic-job1', partition=1}=-915623761775}.
2020-02-26 19:55:00,798 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 2 creating fetcher with offsets {KafkaTopicPartition{topic='input-topic-job1', partition=3}=-915623761775}.
2020-02-26 19:55:00,799 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 2 creating fetcher with offsets {KafkaTopicPartition{topic='input-topic-job1', partition=3}=-915623761775}.
2020-02-26 19:55:00,799 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 1 creating fetcher with offsets {KafkaTopicPartition{topic='input-topic-job1', partition=2}=-915623761775}.
2020-02-26 19:55:00,799 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 3 creating fetcher with offsets {KafkaTopicPartition{topic='input-topic-job1', partition=0}=-915623761775}.
2020-02-26 19:55:00,800 [Legacy Source Thread - Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 1 creating fetcher with offsets {KafkaTopicPartition{topic='input-topic-job1', partition=2}=-915623761775}.
2020-02-26 19:55:00,800 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,804 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,804 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,804 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700804
2020-02-26 19:55:00,805 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,814 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,807 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 0 creating fetcher with offsets {KafkaTopicPartition{topic='input-topic-job1', partition=1}=-915623761775}.
2020-02-26 19:55:00,816 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Subscribed to partition(s): input-topic-job1-2
2020-02-26 19:55:00,820 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,820 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,824 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,827 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input-topic-job1-2
2020-02-26 19:55:00,828 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,828 [Legacy Source Thread - Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (FlinkKafkaConsumerBase.java:688) - Consumer subtask 3 creating fetcher with offsets {KafkaTopicPartition{topic='input-topic-job1', partition=0}=-915623761775}.
2020-02-26 19:55:00,829 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AbstractConfig.java:347) - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = KafkaCsvProducer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-02-26 19:55:00,830 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,830 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,830 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700830
2020-02-26 19:55:00,831 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Subscribed to partition(s): input-topic-job1-1
2020-02-26 19:55:00,831 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input-topic-job1-1
2020-02-26 19:55:00,832 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,832 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,833 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700832
2020-02-26 19:55:00,833 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Subscribed to partition(s): input-topic-job1-0
2020-02-26 19:55:00,834 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input-topic-job1-0
2020-02-26 19:55:00,853 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,856 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,856 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,863 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,863 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,863 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700863
2020-02-26 19:55:00,864 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Subscribed to partition(s): input-topic-job1-3
2020-02-26 19:55:00,864 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input-topic-job1-3
2020-02-26 19:55:00,866 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Resetting offset for partition input-topic-job1-1 to offset 0.
2020-02-26 19:55:00,869 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,869 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,869 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700869
2020-02-26 19:55:00,872 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Subscribed to partition(s): input-topic-job1-0
2020-02-26 19:55:00,873 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input-topic-job1-0
2020-02-26 19:55:00,876 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Resetting offset for partition input-topic-job1-2 to offset 0.
2020-02-26 19:55:00,878 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,883 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,883 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,883 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700883
2020-02-26 19:55:00,887 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Resetting offset for partition input-topic-job1-0 to offset 0.
2020-02-26 19:55:00,888 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,888 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Subscribed to partition(s): input-topic-job1-2
2020-02-26 19:55:00,888 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input-topic-job1-2
2020-02-26 19:55:00,892 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Resetting offset for partition input-topic-job1-0 to offset 0.
2020-02-26 19:55:00,896 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,897 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,897 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700896
2020-02-26 19:55:00,898 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Subscribed to partition(s): input-topic-job1-3
2020-02-26 19:55:00,899 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input-topic-job1-3
2020-02-26 19:55:00,903 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,909 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:117) - Kafka version: 2.4.0
2020-02-26 19:55:00,909 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:118) - Kafka commitId: 77a89fcf8d7fa018
2020-02-26 19:55:00,909 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AppInfoParser.java:119) - Kafka startTimeMs: 1582739700909
2020-02-26 19:55:00,910 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (KafkaConsumer.java:1123) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Subscribed to partition(s): input-topic-job1-1
2020-02-26 19:55:00,910 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (SubscriptionState.java:568) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Seeking to EARLIEST offset of partition input-topic-job1-1
2020-02-26 19:55:00,910 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Resetting offset for partition input-topic-job1-2 to offset 0.
2020-02-26 19:55:00,914 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Resetting offset for partition input-topic-job1-3 to offset 0.
2020-02-26 19:55:00,918 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,918 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (Metadata.java:261) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Cluster ID: 0rOXdrBKStuIl_F5fucA-Q
2020-02-26 19:55:00,924 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Resetting offset for partition input-topic-job1-1 to offset 0.
2020-02-26 19:55:00,927 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (SubscriptionState.java:385) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Resetting offset for partition input-topic-job1-3 to offset 0.
2020-02-26 19:55:00,990 [kafka-producer-network-thread | producer-4] WARN  (NetworkClient.java:1063) - [Producer clientId=producer-4] Error while fetching metadata with correlation id 3 : {_input-topic-job1=LEADER_NOT_AVAILABLE}
2020-02-26 19:55:00,992 [kafka-producer-network-thread | producer-2] WARN  (NetworkClient.java:1063) - [Producer clientId=producer-2] Error while fetching metadata with correlation id 3 : {_input-topic-job1=LEADER_NOT_AVAILABLE}
2020-02-26 19:55:01,001 [kafka-producer-network-thread | producer-1] WARN  (NetworkClient.java:1063) - [Producer clientId=producer-1] Error while fetching metadata with correlation id 3 : {_input-topic-job1=LEADER_NOT_AVAILABLE}
2020-02-26 19:55:01,003 [kafka-producer-network-thread | producer-3] WARN  (NetworkClient.java:1063) - [Producer clientId=producer-3] Error while fetching metadata with correlation id 3 : {_input-topic-job1=LEADER_NOT_AVAILABLE}
2020-02-26 19:55:05,970 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (1/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-13, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-26 19:55:05,975 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (2/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-9, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-26 19:55:05,994 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (4/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-10, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-26 19:55:05,995 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (4/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-16, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-26 19:55:05,999 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (1/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-12, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-26 19:55:06,002 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (2/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-14, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-26 19:55:06,000 [Kafka Fetcher for Source: Custom Source -> Flat Map -> Sink: Unnamed (3/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-11, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
2020-02-26 19:55:06,006 [Kafka Fetcher for Source: Custom Source -> Sink: Print to Std. Out (3/4)] INFO  (AbstractCoordinator.java:756) - [Consumer clientId=consumer-KafkaCsvProducer-15, groupId=KafkaCsvProducer] Discovered group coordinator skl:9092 (id: 2147483647 rack: null)
